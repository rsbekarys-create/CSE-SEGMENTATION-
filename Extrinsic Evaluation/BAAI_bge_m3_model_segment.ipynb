{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZDIBG7fUdf2"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BAAI bge m3 model with segmented dataset"
      ],
      "metadata": {
        "id": "NZ3t_SbZU_Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# SEG_AS_BASE — MORPH_ONLY vs DUAL_VIEW_FUSED (clean+morph)  ✅\n",
        "# LOGIC UNCHANGED\n",
        "#\n",
        "# ✅ Model: BAAI/bge-m3\n",
        "# ✅ Safe CUDA: disable flash/mem-efficient SDP to avoid device-side assert\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
        "# Егер тағы да қате шықса, келесі жолды уақытша қосып debug жасаңыз:\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "import json, re, time, glob, random, csv\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "try:\n",
        "    from bert_score import score as bert_score\n",
        "except Exception:\n",
        "    bert_score = None\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# CONFIG\n",
        "# ---------------------------\n",
        "DATA_PATH   = \"kazakh_segmented_15000.json\"\n",
        "MODEL_NAME  = \"BAAI/bge-m3\"\n",
        "\n",
        "SEED        = 42\n",
        "TEST_SIZE   = 0.10\n",
        "SEM_THR     = 0.85\n",
        "\n",
        "# ✅ MUST run on CUDA (error if not available)\n",
        "DEVICE      = \"cuda\"\n",
        "\n",
        "ALPHA       = 0.50\n",
        "\n",
        "# ✅ memory safety (does NOT change logic)\n",
        "ENC_BATCH   = 16   # қажет болса 8/4 қойыңыз\n",
        "\n",
        "EXPORT_CSV  = True\n",
        "CSV_PATH_MORPH = \"seg_morph_only_details.csv\"\n",
        "CSV_PATH_DUAL  = \"seg_dual_fused_details.csv\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# ✅ SAFE CUDA ATTENTION (very important for bge-m3 on Colab)\n",
        "# ---------------------------\n",
        "def setup_safe_cuda():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"❌ CUDA жоқ. Colab-та GPU қосыңыз: Runtime -> Change runtime type -> GPU.\")\n",
        "\n",
        "    # Disable kernels that often trigger device-side assert on some Colab setups\n",
        "    try:\n",
        "        torch.backends.cuda.enable_flash_sdp(False)\n",
        "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "        torch.backends.cuda.enable_math_sdp(True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Optional stability knobs\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utility: file auto-find\n",
        "# ---------------------------\n",
        "def find_data_path(p: str) -> str:\n",
        "    if Path(p).exists():\n",
        "        return p\n",
        "    candidates = [f\"/content/{p}\", f\"/content/drive/MyDrive/{p}\"]\n",
        "    for c in candidates:\n",
        "        if Path(c).exists():\n",
        "            return c\n",
        "    name = Path(p).name\n",
        "    hits = glob.glob(f\"**/{name}\", recursive=True)\n",
        "    if hits:\n",
        "        return hits[0]\n",
        "    near = glob.glob(\"**/*.json\", recursive=True)\n",
        "    raise FileNotFoundError(\n",
        "        f\"❌ File not found: {p}\\nPWD: {Path.cwd()}\\n\"\n",
        "        f\"Found .json (first 30):\\n\" + \"\\n\".join(near[:30])\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Robust loader (JSON array / JSONL / brace-scan)\n",
        "# ---------------------------\n",
        "def load_qa_records(path: str) -> List[Dict[str, str]]:\n",
        "    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "    if not text:\n",
        "        raise ValueError(f\"Файл бос: {path}\")\n",
        "\n",
        "    if text[0] == \"[\":\n",
        "        try:\n",
        "            return _normalize_records(json.loads(text))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    lines = [ln.strip().rstrip(\",\") for ln in text.splitlines() if ln.strip()]\n",
        "    if lines and lines[0].startswith(\"{\"):\n",
        "        recs, ok = [], True\n",
        "        for ln in lines:\n",
        "            try:\n",
        "                recs.append(json.loads(ln))\n",
        "            except Exception:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok and recs:\n",
        "            return _normalize_records(recs)\n",
        "\n",
        "    objs, buf, depth = [], [], 0\n",
        "    in_str, esc, started = False, False, False\n",
        "\n",
        "    for ch in text:\n",
        "        if not started:\n",
        "            if ch == \"{\":\n",
        "                started = True\n",
        "                depth = 1\n",
        "                buf = [\"{\"]\n",
        "            continue\n",
        "\n",
        "        buf.append(ch)\n",
        "\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif ch == \"\\\\\":\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if ch == '\"':\n",
        "                in_str = True\n",
        "            elif ch == \"{\":\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    obj_txt = \"\".join(buf)\n",
        "                    buf = []\n",
        "                    started = False\n",
        "                    try:\n",
        "                        objs.append(json.loads(obj_txt))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "    if not objs:\n",
        "        raise ValueError(f\"JSON оқу мүмкін болмады. Файл форматын тексеріңіз: {path}\")\n",
        "    return _normalize_records(objs)\n",
        "\n",
        "\n",
        "def _normalize_records(data: Any) -> List[Dict[str, str]]:\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(\"Дерек list болуы керек.\")\n",
        "    out = []\n",
        "    for x in data:\n",
        "        if not isinstance(x, dict):\n",
        "            continue\n",
        "        q = x.get(\"question\") or x.get(\"instruction\") or \"\"\n",
        "        a = x.get(\"answer\") or x.get(\"response\") or \"\"\n",
        "        q = str(q).strip()\n",
        "        a = str(a).strip()\n",
        "        if q and a:\n",
        "            out.append({\"question\": q, \"answer\": a})\n",
        "    if not out:\n",
        "        raise ValueError(\"question/answer табылмады немесе бос.\")\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Text normalization\n",
        "# ---------------------------\n",
        "_punct_space_left  = re.compile(r\"\\s+([.,!?;:%)\\]\\}])\")\n",
        "_punct_space_right = re.compile(r\"([(\\[\\{])\\s+\")\n",
        "_multi_space       = re.compile(r\"\\s+\")\n",
        "\n",
        "def _norm_space_punct(t: str) -> str:\n",
        "    t = t.replace(\" - \", \"-\")\n",
        "    t = _punct_space_left.sub(r\"\\1\", t)\n",
        "    t = _punct_space_right.sub(r\"\\1\", t)\n",
        "    t = _multi_space.sub(\" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def morph_marker_view(text: str) -> str:\n",
        "    t = \"\" if text is None else str(text)\n",
        "    return _norm_space_punct(t)\n",
        "\n",
        "def clean_view(text: str) -> str:\n",
        "    t = \"\" if text is None else str(text)\n",
        "    t = t.replace(\"@@ \", \"\").replace(\"@@\", \"\")\n",
        "    return _norm_space_punct(t)\n",
        "\n",
        "def norm_for_exact(text: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", morph_marker_view(text).lower()).strip()\n",
        "\n",
        "def tokens(text: str) -> List[str]:\n",
        "    t = morph_marker_view(text).lower()\n",
        "    return re.findall(r\"[a-zA-Zа-яА-ЯәғқңөұүһіӘҒҚҢӨÚÜҺІ0-9]+\", t)\n",
        "\n",
        "def token_f1(pred: str, gold: str) -> float:\n",
        "    p = tokens(pred); g = tokens(gold)\n",
        "    if not p and not g: return 1.0\n",
        "    if not p or not g: return 0.0\n",
        "    from collections import Counter\n",
        "    pc = Counter(p); gc = Counter(g)\n",
        "    inter = sum((pc & gc).values())\n",
        "    if inter == 0: return 0.0\n",
        "    prec = inter / max(1, len(p))\n",
        "    rec  = inter / max(1, len(g))\n",
        "    return (2 * prec * rec) / (prec + rec + 1e-12)\n",
        "\n",
        "def clean_out_answer(text: str) -> str:\n",
        "    t = \"\" if text is None else str(text)\n",
        "    t = t.replace(\"@@ \", \"\").replace(\"@@\", \"\")\n",
        "    return _norm_space_punct(t)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Embedding helpers\n",
        "# ---------------------------\n",
        "def _l2norm(x: np.ndarray) -> np.ndarray:\n",
        "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
        "    return x / n\n",
        "\n",
        "def fuse_embeddings(e_clean: np.ndarray, e_morph: np.ndarray, alpha: float) -> np.ndarray:\n",
        "    fused = alpha * e_clean + (1.0 - alpha) * e_morph\n",
        "    if fused.ndim == 1:\n",
        "        return fused / (np.linalg.norm(fused) + 1e-12)\n",
        "    return _l2norm(fused)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Retrieval index\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class QAIndex:\n",
        "    mode: str\n",
        "    q_text: List[str]\n",
        "    q_emb: np.ndarray\n",
        "    ans_text: List[str]\n",
        "    a_emb: np.ndarray\n",
        "\n",
        "def _encode(model: SentenceTransformer, texts: List[str]) -> np.ndarray:\n",
        "    # helper: same encoding settings everywhere (logic unchanged)\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=False,\n",
        "        batch_size=ENC_BATCH\n",
        "    )\n",
        "\n",
        "def build_index_morph_only(model: SentenceTransformer, train_rows: List[Dict[str,str]]) -> QAIndex:\n",
        "    q_view = [morph_marker_view(x[\"question\"]) for x in train_rows]\n",
        "    a_view = [morph_marker_view(x[\"answer\"])   for x in train_rows]\n",
        "    q_emb = _encode(model, q_view)\n",
        "    a_emb = _encode(model, a_view)\n",
        "    return QAIndex(\"MORPH_ONLY\", q_view, q_emb, a_view, a_emb)\n",
        "\n",
        "def build_index_dual_fused(model: SentenceTransformer, train_rows: List[Dict[str,str]], alpha: float) -> QAIndex:\n",
        "    q_clean = [clean_view(x[\"question\"]) for x in train_rows]\n",
        "    q_morph = [morph_marker_view(x[\"question\"]) for x in train_rows]\n",
        "    a_view  = [morph_marker_view(x[\"answer\"]) for x in train_rows]\n",
        "\n",
        "    e_clean = _encode(model, q_clean)\n",
        "    e_morph = _encode(model, q_morph)\n",
        "    q_fused = fuse_embeddings(e_clean, e_morph, alpha)\n",
        "\n",
        "    a_emb = _encode(model, a_view)\n",
        "\n",
        "    q_text = [f\"CLEAN||MORPH: {qc} || {qm}\" for qc, qm in zip(q_clean, q_morph)]\n",
        "    return QAIndex(f\"DUAL_FUSED(alpha={alpha})\", q_text, q_fused, a_view, a_emb)\n",
        "\n",
        "def retrieve_top1(index: QAIndex, q_vec: np.ndarray) -> Tuple[int, float]:\n",
        "    sims = np.dot(index.q_emb, q_vec)\n",
        "    i = int(np.argmax(sims))\n",
        "    return i, float(sims[i])\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# BERTScore helper\n",
        "# ---------------------------\n",
        "def _bert_lang_try(preds: List[str], golds: List[str]) -> Optional[float]:\n",
        "    if bert_score is None:\n",
        "        return None\n",
        "    for lang in (\"kk\", \"tr\", \"en\"):\n",
        "        try:\n",
        "            P, R, F1 = bert_score(preds, golds, lang=lang, rescale_with_baseline=True)\n",
        "            arr = F1.numpy() if hasattr(F1, \"numpy\") else np.array(F1)\n",
        "            return float(np.mean(arr))\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation\n",
        "# ---------------------------\n",
        "def eval_with_index(model: SentenceTransformer, index: QAIndex, test_rows: List[Dict[str,str]], alpha: float):\n",
        "    if index.mode.startswith(\"MORPH_ONLY\"):\n",
        "        test_q = [morph_marker_view(x[\"question\"]) for x in test_rows]\n",
        "        test_q_emb = _encode(model, test_q)\n",
        "    else:\n",
        "        tq_clean = [clean_view(x[\"question\"]) for x in test_rows]\n",
        "        tq_morph = [morph_marker_view(x[\"question\"]) for x in test_rows]\n",
        "        e_clean = _encode(model, tq_clean)\n",
        "        e_morph = _encode(model, tq_morph)\n",
        "        test_q_emb = fuse_embeddings(e_clean, e_morph, alpha)\n",
        "        test_q = tq_morph\n",
        "\n",
        "    gold = [morph_marker_view(x[\"answer\"]) for x in test_rows]\n",
        "    gold_a_emb = _encode(model, gold)\n",
        "\n",
        "    exacts, tf1s, qcos1s, semhit = [], [], [], []\n",
        "    preds_for_bert, golds_for_bert = [], []\n",
        "    details = []\n",
        "\n",
        "    for i in range(len(test_rows)):\n",
        "        idx, qcos = retrieve_top1(index, test_q_emb[i])\n",
        "        pred = index.ans_text[idx]\n",
        "        g    = gold[i]\n",
        "\n",
        "        ex = 1.0 if norm_for_exact(pred) == norm_for_exact(g) else 0.0\n",
        "        f1 = token_f1(pred, g)\n",
        "        qsim = float(qcos)\n",
        "\n",
        "        sem_cos = float(np.dot(index.a_emb[idx], gold_a_emb[i]))\n",
        "        sh = 1.0 if sem_cos >= SEM_THR else 0.0\n",
        "\n",
        "        exacts.append(ex); tf1s.append(f1); qcos1s.append(qsim); semhit.append(sh)\n",
        "\n",
        "        if bert_score is not None:\n",
        "            preds_for_bert.append(pred)\n",
        "            golds_for_bert.append(g)\n",
        "\n",
        "        details.append({\n",
        "            \"mode\": index.mode,\n",
        "            \"test_question\": test_q[i],\n",
        "            \"gold_answer\": g,\n",
        "            \"pred_answer\": pred,\n",
        "            \"QSim\": qsim,\n",
        "            \"Exact\": ex,\n",
        "            \"TokenF1\": f1,\n",
        "            \"AnsCos\": sem_cos,\n",
        "            \"SemHit\": sh\n",
        "        })\n",
        "\n",
        "    out = {\n",
        "        \"Mode\": index.mode,\n",
        "        \"Exact@1\": float(np.mean(exacts)),\n",
        "        \"TokenF1@1\": float(np.mean(tf1s)),\n",
        "        \"MeanCos@1(QSim)\": float(np.mean(qcos1s)),\n",
        "        f\"Semantic@1(ans_cos≥{SEM_THR})\": float(np.mean(semhit)),\n",
        "    }\n",
        "\n",
        "    if bert_score is not None and preds_for_bert:\n",
        "        bf1 = _bert_lang_try(preds_for_bert, golds_for_bert)\n",
        "        if bf1 is not None:\n",
        "            out[\"BERTScoreF1@1\"] = float(bf1)\n",
        "\n",
        "    return out, details\n",
        "\n",
        "\n",
        "def print_result_table(rows: List[Dict[str,Any]]):\n",
        "    print(\"\\n==================== RESULTS (MORPH_ONLY vs DUAL_FUSED) ====================\")\n",
        "    keys = []\n",
        "    for r in rows:\n",
        "        for k in r.keys():\n",
        "            if k not in keys:\n",
        "                keys.append(k)\n",
        "\n",
        "    for r in rows:\n",
        "        print(\"\\n---\", r.get(\"Mode\", \"MODE\"), \"---\")\n",
        "        for k in keys:\n",
        "            if k not in r:\n",
        "                continue\n",
        "            v = r[k]\n",
        "            if isinstance(v, float):\n",
        "                print(f\"{k:>28}: {v:.6f}\")\n",
        "            else:\n",
        "                print(f\"{k:>28}: {v}\")\n",
        "\n",
        "def export_csv(details: List[Dict[str,Any]], path: str):\n",
        "    if not details:\n",
        "        return\n",
        "    fields = list(details[0].keys())\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fields)\n",
        "        w.writeheader()\n",
        "        for r in details:\n",
        "            w.writerow(r)\n",
        "    print(f\"\\n✅ CSV exported: {path}  (rows={len(details)})\")\n",
        "\n",
        "\n",
        "def interactive(model: SentenceTransformer, idx_morph: QAIndex, idx_dual: QAIndex, alpha: float):\n",
        "    mode = \"DUAL\"\n",
        "    print(\"\\n==================== INTERACTIVE QA (SEG_AS_BASE) ====================\")\n",
        "    print(\"Commands: /morph  -> MORPH_ONLY mode\")\n",
        "    print(\"          /dual   -> DUAL_FUSED mode\")\n",
        "    print(\"          exit    -> finish and run metrics\\n\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"Сұрақ: \").strip()\n",
        "        if not q:\n",
        "            continue\n",
        "        if q.lower() in {\"exit\",\"quit\",\"q\"}:\n",
        "            break\n",
        "        if q.lower() == \"/morph\":\n",
        "            mode = \"MORPH\"\n",
        "            print(\"✅ Switched to MORPH_ONLY\\n\")\n",
        "            continue\n",
        "        if q.lower() == \"/dual\":\n",
        "            mode = \"DUAL\"\n",
        "            print(f\"✅ Switched to DUAL_FUSED(alpha={alpha})\\n\")\n",
        "            continue\n",
        "\n",
        "        if mode == \"MORPH\":\n",
        "            qv = _encode(model, [morph_marker_view(q)])[0]\n",
        "            j, sim = retrieve_top1(idx_morph, qv)\n",
        "            print(f\"\\n[{idx_morph.mode}] Top1 QSim={sim:.4f}\\n{clean_out_answer(idx_morph.ans_text[j])}\\n\")\n",
        "        else:\n",
        "            qc = clean_view(q)\n",
        "            qm = morph_marker_view(q)\n",
        "            ec = _encode(model, [qc])[0]\n",
        "            em = _encode(model, [qm])[0]\n",
        "            qf = fuse_embeddings(ec, em, alpha)\n",
        "            j, sim = retrieve_top1(idx_dual, qf)\n",
        "            print(f\"\\n[{idx_dual.mode}] Top1 QSim={sim:.4f}\\n{clean_out_answer(idx_dual.ans_text[j])}\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "    setup_safe_cuda()\n",
        "\n",
        "    data_path = find_data_path(DATA_PATH)\n",
        "    rows = load_qa_records(data_path)\n",
        "    print(f\"[SEG_AS_BASE] Loaded: {len(rows)} | {data_path}\")\n",
        "\n",
        "    train_rows, test_rows = train_test_split(rows, test_size=TEST_SIZE, random_state=SEED, shuffle=True)\n",
        "    print(\"\\n==================== ONE SPLIT ====================\")\n",
        "    print(f\"Total={len(rows)} | Train={len(train_rows)} | Test={len(test_rows)} | seed={SEED} | test_size={TEST_SIZE}\")\n",
        "    print(\"Mode A = MORPH_ONLY (keeps @@)\")\n",
        "    print(f\"Mode B = DUAL_FUSED(clean+morph), alpha={ALPHA}\")\n",
        "\n",
        "    # ✅ Important: enforce safer attention impl via model_kwargs when possible\n",
        "    # sentence-transformers will pass this to transformers AutoModel in recent versions\n",
        "    try:\n",
        "        model = SentenceTransformer(\n",
        "            MODEL_NAME,\n",
        "            device=DEVICE,\n",
        "            model_kwargs={\"attn_implementation\": \"eager\"}\n",
        "        )\n",
        "    except TypeError:\n",
        "        # older sentence-transformers: fallback (still benefits from disabling SDP above)\n",
        "        model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
        "\n",
        "    print(f\"\\nModel: {MODEL_NAME}\")\n",
        "    print(f\"Device: {DEVICE} | CUDA available: {torch.cuda.is_available()} | batch={ENC_BATCH}\")\n",
        "\n",
        "    print(\"\\n[1/3] Building MORPH_ONLY index...\")\n",
        "    idx_morph = build_index_morph_only(model, train_rows)\n",
        "\n",
        "    print(\"[2/3] Building DUAL_FUSED index...\")\n",
        "    idx_dual = build_index_dual_fused(model, train_rows, ALPHA)\n",
        "\n",
        "    interactive(model, idx_morph, idx_dual, ALPHA)\n",
        "\n",
        "    print(\"\\n[3/3] Running evaluation...\")\n",
        "    t0 = time.time()\n",
        "    res_m, det_m = eval_with_index(model, idx_morph, test_rows, ALPHA)\n",
        "    res_d, det_d = eval_with_index(model, idx_dual,  test_rows, ALPHA)\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    print_result_table([res_m, res_d])\n",
        "    print(f\"\\nTime: {dt:.2f}s\")\n",
        "    if bert_score is None:\n",
        "        print(\"Note: BERTScore орнатылмаған (pip install bert-score).\")\n",
        "\n",
        "    if EXPORT_CSV:\n",
        "        export_csv(det_m, CSV_PATH_MORPH)\n",
        "        export_csv(det_d, CSV_PATH_DUAL)\n",
        "\n",
        "    print(\"\\n✅ DONE: MORPH_ONLY vs DUAL_FUSED finished (same split, same seed)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iWd7ssVbUlbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}