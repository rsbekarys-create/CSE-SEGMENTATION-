{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hRPxMK3t71JV",
        "outputId": "c35e26b4-ac7a-4eb7-c94b-721dfdaff60d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu128)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (5.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (1.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.3.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2026.1.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=3.0.0->bert-score) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=3.0.0->bert-score) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=3.0.0->bert-score) (1.5.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers>=3.0.0->bert-score) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers>=3.0.0->bert-score) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers>=3.0.0->bert-score) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers>=3.0.0->bert-score) (0.16.0)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert-score\n",
            "Successfully installed bert-score-0.3.13\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.24.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.46)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.52.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-score\n",
        "!pip install datasets\n",
        "!pip install sentence-transformers datasets accelerate transformers wandb\n",
        "!pip install sentence-transformers torch sklearn bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft -q"
      ],
      "metadata": {
        "id": "xY8jur6CejEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline BAAI/bge-m3 model"
      ],
      "metadata": {
        "id": "Im3hGTFOPRVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# CLEAN QA → Fine-tune + Evaluation\n",
        "# VERSION 5 — ТАЗА ДАТАСЕТ (сегменттелмеген)\n",
        "#\n",
        "# ӨЗГЕРІСТЕР (v4 → v5):\n",
        "#   - DATA_PATH  → baseline_15000.json  (таза, @@-сіз)\n",
        "#   - seg_train  жойылды → тек cln_train қолданылады\n",
        "#   - clean_segmented_text() тек қауіпсіздік үшін сақталды\n",
        "#     (таза деректе @@ жоқ болса да зиян жоқ)\n",
        "#   - train_examples: cln_q + cln_a  (таза мәтін)\n",
        "#   - Барлық басқа логика v4-пен БІРДЕЙ\n",
        "#\n",
        "# ӨЗГЕРІСТЕР (v5 → v5-bge):\n",
        "#   - MODEL_NAME  → BAAI/bge-m3\n",
        "#   - OUTPUT_PATH → fine-tuned-kz-model-bge-m3\n",
        "#   - encode_query: BGE-M3 prefix қажет етпейді → prefix алынды\n",
        "#   - encode_passage: өзгеріссіз (prefix жоқ болатын)\n",
        "#   - train_examples: e5_wrap_query() жойылды → cln_q тікелей\n",
        "#   - detect_lora_targets(): авто-анықтау функциясы қосылды\n",
        "#\n",
        "# Metrics:\n",
        "#   Exact@1 | TokenF1@1 | MeanCos@1(QSim)\n",
        "#   Semantic@1(ans_cos>=0.85) | BERTScoreF1@1 (optional)\n",
        "#\n",
        "# Requires: pip install peft\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"]              = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]      = \"false\"\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]     = \"expandable_segments:True\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time, re, json, glob, random\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# ── PEFT / LoRA ──────────────────────────────────────────────\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    HAS_PEFT = True\n",
        "except ImportError:\n",
        "    HAS_PEFT = False\n",
        "    print(\"⚠️  peft not installed. Run:  pip install peft\")\n",
        "    print(\"    Falling back to frozen-backbone training.\")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# BERTScore — optional\n",
        "try:\n",
        "    from bert_score import score as bert_score_fn\n",
        "    HAS_BERT_SCORE = True\n",
        "except ImportError:\n",
        "    bert_score_fn  = None\n",
        "    HAS_BERT_SCORE = False\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CONFIG\n",
        "# ================================================================\n",
        "DATA_PATH   = \"baseline_15000.json\"\n",
        "\n",
        "# ✅ ӨЗГЕРІС 1: Модель ауыстырылды\n",
        "MODEL_NAME  = \"BAAI/bge-m3\"                  # бұрын: intfloat/multilingual-e5-large-instruct\n",
        "OUTPUT_PATH = \"fine-tuned-kz-model-bge-m3\"   # бұрын: fine-tuned-kz-model-e5-large\n",
        "\n",
        "TEST_SIZE    = 0.4\n",
        "EPOCHS       = 3\n",
        "WARMUP_STEPS = 100\n",
        "BATCH_SIZE   = 16\n",
        "SEM_THR      = 0.85\n",
        "SEED         = 42\n",
        "USE_FP16     = torch.cuda.is_available()\n",
        "\n",
        "LORA_R       = 16\n",
        "LORA_ALPHA   = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGETS = [\"query\", \"value\"]\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# REPRODUCIBILITY\n",
        "# ================================================================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FILE UTILITIES\n",
        "# ================================================================\n",
        "def find_data_path(p: str) -> str:\n",
        "    if Path(p).exists():\n",
        "        return p\n",
        "    for c in [f\"/content/{p}\", f\"/content/drive/MyDrive/{p}\"]:\n",
        "        if Path(c).exists():\n",
        "            return c\n",
        "    name = Path(p).name\n",
        "    hits = glob.glob(f\"**/{name}\", recursive=True)\n",
        "    if hits:\n",
        "        return hits[0]\n",
        "    near = glob.glob(\"**/*.json\", recursive=True)\n",
        "    raise FileNotFoundError(\n",
        "        f\"File not found: {p}\\nPWD: {Path.cwd()}\\n\"\n",
        "        f\"Nearby .json files:\\n\" + \"\\n\".join(near[:30])\n",
        "    )\n",
        "\n",
        "\n",
        "def _normalize_records(data: list) -> list:\n",
        "    out = []\n",
        "    for x in data:\n",
        "        if not isinstance(x, dict):\n",
        "            continue\n",
        "        q = x.get(\"question\") or x.get(\"instruction\") or \"\"\n",
        "        a = x.get(\"answer\")   or x.get(\"response\")    or \"\"\n",
        "        q, a = str(q).strip(), str(a).strip()\n",
        "        if q and a:\n",
        "            out.append({\"question\": q, \"answer\": a})\n",
        "    if not out:\n",
        "        raise ValueError(\"No valid question/answer pairs found.\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_qa_records(path: str) -> list:\n",
        "    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "    if not text:\n",
        "        raise ValueError(f\"File is empty: {path}\")\n",
        "\n",
        "    if text[0] == \"[\":\n",
        "        try:\n",
        "            return _normalize_records(json.loads(text))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    lines = [ln.strip().rstrip(\",\") for ln in text.splitlines() if ln.strip()]\n",
        "    if lines and lines[0].startswith(\"{\"):\n",
        "        recs, ok = [], True\n",
        "        for ln in lines:\n",
        "            try:\n",
        "                recs.append(json.loads(ln))\n",
        "            except Exception:\n",
        "                ok = False; break\n",
        "        if ok and recs:\n",
        "            return _normalize_records(recs)\n",
        "\n",
        "    objs, buf, depth = [], [], 0\n",
        "    in_str = esc = started = False\n",
        "    for ch in text:\n",
        "        if not started:\n",
        "            if ch == \"{\":\n",
        "                started, depth, buf = True, 1, [\"{\"]\n",
        "            continue\n",
        "        buf.append(ch)\n",
        "        if in_str:\n",
        "            if esc:          esc = False\n",
        "            elif ch == \"\\\\\": esc = True\n",
        "            elif ch == '\"':  in_str = False\n",
        "        else:\n",
        "            if   ch == '\"': in_str = True\n",
        "            elif ch == \"{\": depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    try:\n",
        "                        objs.append(json.loads(\"\".join(buf)))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    buf, started = [], False\n",
        "\n",
        "    if not objs:\n",
        "        raise ValueError(f\"Could not parse JSON. Preview:\\n{text[:400]}\")\n",
        "    return _normalize_records(objs)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TEXT UTILITIES\n",
        "# ================================================================\n",
        "def clean_segmented_text(text: str) -> str:\n",
        "    # ✅ Таза деректе @@ болмайды, бірақ қауіпсіздік үшін сақтаймыз\n",
        "    t = re.sub(r\"@@\\s*\", \"\", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    t = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", t)\n",
        "    t = re.sub(r\"\\s*([-/])\\s*\", r\"\\1\", t)\n",
        "    return t\n",
        "\n",
        "\n",
        "def normalize_text(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", str(t).lower().strip())\n",
        "\n",
        "\n",
        "KZ_PAT = re.compile(\n",
        "    r\"[a-zA-Zа-яА-ЯәғқңөұүһіӘҒҚҢӨҰҮҺІ0-9]+\"\n",
        ")\n",
        "\n",
        "def tokens(text: str) -> list:\n",
        "    return KZ_PAT.findall(normalize_text(text))\n",
        "\n",
        "\n",
        "def token_f1(pred: str, gold: str) -> float:\n",
        "    p, g = tokens(pred), tokens(gold)\n",
        "    if not p and not g: return 1.0\n",
        "    if not p or  not g: return 0.0\n",
        "    pc, gc = Counter(p), Counter(g)\n",
        "    inter  = sum((pc & gc).values())\n",
        "    if inter == 0: return 0.0\n",
        "    prec = inter / len(p)\n",
        "    rec  = inter / len(g)\n",
        "    return (2 * prec * rec) / (prec + rec + 1e-12)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# BGE-M3 ENCODING\n",
        "#\n",
        "# ✅ ӨЗГЕРІС 2: BGE-M3 — prefix ҚАЖЕТ ЕМЕС (екі жақта да)\n",
        "#\n",
        "#   E5-instruct (бұрынғы):\n",
        "#     query   → \"Instruct: ...\\nQuery: {text}\"  (міндетті prefix)\n",
        "#     passage → тікелей мәтін\n",
        "#\n",
        "#   BGE-M3 (қазіргі):\n",
        "#     query   → тікелей мәтін  (prefix жоқ)\n",
        "#     passage → тікелей мәтін  (prefix жоқ)\n",
        "#\n",
        "#   BAAI ресми нұсқаулығы:\n",
        "#     \"No instruction needed for either queries or passages.\"\n",
        "# ================================================================\n",
        "\n",
        "def _nan_guard(vecs: np.ndarray, label: str) -> np.ndarray:\n",
        "    bad = ~np.isfinite(vecs).all(axis=1)\n",
        "    if bad.any():\n",
        "        print(f\"  ⚠ {bad.sum()} {label} vectors had NaN/Inf → zeroed.\")\n",
        "        vecs[bad] = 0.0\n",
        "    return vecs\n",
        "\n",
        "def encode_query(mdl: SentenceTransformer, texts: list) -> np.ndarray:\n",
        "    # ✅ BGE-M3: e5_wrap_query() жоқ — мәтін тікелей\n",
        "    vecs = mdl.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=False,\n",
        "    )\n",
        "    return _nan_guard(vecs, \"query\")\n",
        "\n",
        "def encode_passage(mdl: SentenceTransformer, texts: list) -> np.ndarray:\n",
        "    vecs = mdl.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=False,\n",
        "    )\n",
        "    return _nan_guard(vecs, \"passage\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# LOAD DATA\n",
        "# ================================================================\n",
        "data_path = find_data_path(DATA_PATH)\n",
        "rows      = load_qa_records(data_path)\n",
        "\n",
        "clean_data = pd.DataFrame(rows)\n",
        "clean_data[\"question\"] = clean_data[\"question\"].apply(clean_segmented_text)\n",
        "clean_data[\"answer\"]   = clean_data[\"answer\"].apply(clean_segmented_text)\n",
        "\n",
        "print(f\"[LOADED] rows={len(clean_data)} | path={data_path}\")\n",
        "print(f\"[DATA]   Үлгі сұрақ : {clean_data['question'].iloc[0][:80]}\")\n",
        "print(f\"[DATA]   Үлгі жауап : {clean_data['answer'].iloc[0][:80]}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TRAIN / TEST SPLIT\n",
        "# ================================================================\n",
        "idx = np.arange(len(clean_data))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    idx, test_size=TEST_SIZE, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "cln_train = clean_data.iloc[train_idx].reset_index(drop=True)\n",
        "cln_test  = clean_data.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"[SPLIT] train={len(cln_train)} | test={len(cln_test)}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# LOAD BASE MODEL\n",
        "# ================================================================\n",
        "print(f\"\\n[MODEL] Loading {MODEL_NAME} ...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# LoRA TARGET MODULES АВТОМАТТЫ АНЫҚТАУ\n",
        "#\n",
        "# ✅ ӨЗГЕРІС 3: BGE-M3 нұсқасына байланысты attention layer\n",
        "#    атаулары \"query\"/\"value\" немесе \"q_proj\"/\"v_proj\" болуы мүмкін.\n",
        "#    Функция backbone графын тексеріп дұрыс атауды өзі табады.\n",
        "# ================================================================\n",
        "def detect_lora_targets(st_model: SentenceTransformer) -> list:\n",
        "    backbone     = st_model[0].auto_model\n",
        "    linear_names = set()\n",
        "    for name, module in backbone.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            linear_names.add(name.split(\".\")[-1])\n",
        "\n",
        "    for pair in [[\"query\", \"value\"], [\"q_proj\", \"v_proj\"], [\"q\", \"v\"]]:\n",
        "        if all(t in linear_names for t in pair):\n",
        "            print(f\"[LoRA] Detected target modules: {pair}\")\n",
        "            return pair\n",
        "\n",
        "    fallback = list(linear_names)[:4]\n",
        "    print(f\"[LoRA] Standard targets not found. Fallback: {fallback}\")\n",
        "    return fallback\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# APPLY LoRA TO THE BACKBONE\n",
        "# ================================================================\n",
        "def apply_lora(st_model: SentenceTransformer) -> SentenceTransformer:\n",
        "    backbone = st_model[0].auto_model\n",
        "\n",
        "    # ✅ ӨЗГЕРІС 4: Hardcode LORA_TARGETS орнына авто-анықтау\n",
        "    detected_targets = detect_lora_targets(st_model)\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        task_type=TaskType.FEATURE_EXTRACTION,\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        target_modules=detected_targets,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    peft_backbone = get_peft_model(backbone, lora_cfg)\n",
        "\n",
        "    total  = sum(p.numel() for p in peft_backbone.parameters())\n",
        "    train_ = sum(p.numel() for p in peft_backbone.parameters() if p.requires_grad)\n",
        "    print(f\"[LoRA] Total params : {total/1e6:.1f}M\")\n",
        "    print(f\"[LoRA] Trainable    : {train_/1e6:.2f}M  ({100*train_/total:.2f}% of total)\")\n",
        "\n",
        "    st_model[0].auto_model = peft_backbone\n",
        "    return st_model\n",
        "\n",
        "\n",
        "def freeze_backbone(st_model: SentenceTransformer) -> SentenceTransformer:\n",
        "    for param in st_model[0].auto_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    total  = sum(p.numel() for p in st_model.parameters())\n",
        "    train_ = sum(p.numel() for p in st_model.parameters() if p.requires_grad)\n",
        "    print(f\"[FROZEN] Trainable: {train_/1e6:.2f}M / {total/1e6:.1f}M total\")\n",
        "    return st_model\n",
        "\n",
        "\n",
        "if HAS_PEFT:\n",
        "    model = apply_lora(model)\n",
        "    print(\"[LoRA] Adapters applied. Backbone frozen.\")\n",
        "else:\n",
        "    model = freeze_backbone(model)\n",
        "    print(\"[FROZEN] Backbone frozen (install peft for better results).\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    alloc      = torch.cuda.memory_allocated() / 1e9\n",
        "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"[GPU] After model prep: {alloc:.2f} GB / {total_vram:.1f} GB\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# BUILD TRAINING EXAMPLES\n",
        "# ✅ ӨЗГЕРІС 5: e5_wrap_query() жойылды → cln_q тікелей беріледі\n",
        "# ================================================================\n",
        "print(\"\\n[TRAIN] Building training examples ...\")\n",
        "train_examples   = []\n",
        "n_train          = len(cln_train)\n",
        "all_cln_answers  = cln_train[\"answer\"].tolist()\n",
        "\n",
        "for i in range(n_train):\n",
        "    cln_q = str(cln_train.loc[i, \"question\"]).strip()\n",
        "    cln_a = str(cln_train.loc[i, \"answer\"]).strip()\n",
        "    if not cln_q or not cln_a:\n",
        "        continue\n",
        "    neg_idx = random.choice([k for k in range(n_train) if k != i])\n",
        "    neg_a   = str(all_cln_answers[neg_idx]).strip()\n",
        "    # ✅ BGE-M3: prefix жоқ — cln_q тікелей\n",
        "    train_examples.append(\n",
        "        InputExample(texts=[cln_q, cln_a, neg_a])\n",
        "    )\n",
        "\n",
        "if len(train_examples) < 2:\n",
        "    raise ValueError(\"Not enough training pairs.\")\n",
        "\n",
        "print(f\"[TRAIN] {len(train_examples)} examples | batch_size={BATCH_SIZE}\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    train_loss = losses.CachedMultipleNegativesRankingLoss(\n",
        "        model, mini_batch_size=8\n",
        "    )\n",
        "    print(\"[LOSS] CachedMultipleNegativesRankingLoss\")\n",
        "except AttributeError:\n",
        "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "    print(\"[LOSS] MultipleNegativesRankingLoss\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CLEAR VRAM BEFORE TRAINING\n",
        "# ================================================================\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    free = (torch.cuda.get_device_properties(0).total_memory\n",
        "            - torch.cuda.memory_allocated()) / 1e9\n",
        "    print(f\"\\n[GPU] Free VRAM before fit: {free:.2f} GB\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FINE-TUNING\n",
        "# ================================================================\n",
        "print(f\"\\n[FINE-TUNE] Epochs={EPOCHS} | warmup={WARMUP_STEPS} | fp16={USE_FP16}\")\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    show_progress_bar=True,\n",
        "    use_amp=USE_FP16,\n",
        "    optimizer_params={\"lr\": 2e-4},\n",
        ")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# MERGE LoRA WEIGHTS & SAVE\n",
        "# ================================================================\n",
        "if HAS_PEFT:\n",
        "    print(\"\\n[LoRA] Merging adapter weights into base model ...\")\n",
        "    try:\n",
        "        model[0].auto_model = model[0].auto_model.merge_and_unload()\n",
        "        print(\"[LoRA] Merge complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[LoRA] Merge failed ({e}) — saving with adapter weights.\")\n",
        "\n",
        "model.save(OUTPUT_PATH)\n",
        "print(f\"[SAVED] {OUTPUT_PATH}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "model = SentenceTransformer(OUTPUT_PATH)\n",
        "print(\"[LOADED] Model reloaded from disk.\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# BUILD KB INDEX\n",
        "# ================================================================\n",
        "train_questions = cln_train[\"question\"].tolist()\n",
        "train_answers   = cln_train[\"answer\"].tolist()\n",
        "\n",
        "print(f\"\\n[KB] Indexing {len(train_questions)} training questions ...\")\n",
        "train_q_embeds = encode_query(model, train_questions)\n",
        "print(f\"[KB] Index shape: {train_q_embeds.shape}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# RETRIEVAL\n",
        "# ================================================================\n",
        "def ask_question(question: str, threshold: float = 0.0):\n",
        "    q_clean    = clean_segmented_text(question)\n",
        "    q_vec      = encode_query(model, [q_clean])\n",
        "    sims       = q_vec @ train_q_embeds.T\n",
        "    best_idx   = int(np.argmax(sims[0]))\n",
        "    best_score = float(sims[0][best_idx])\n",
        "\n",
        "    if not np.isfinite(best_score):\n",
        "        return \"Кешіріңіз, нақты жауап табылмады.\", -1, 0.0\n",
        "    if best_score < threshold:\n",
        "        return \"Кешіріңіз, нақты жауап табылмады.\", -1, best_score\n",
        "    return train_answers[best_idx], best_idx, best_score\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# EVALUATION\n",
        "# ================================================================\n",
        "def evaluate_system() -> dict:\n",
        "    print(\"\\n[EVAL] Running ...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    gold_qs = cln_test[\"question\"].tolist()\n",
        "    gold_as = cln_test[\"answer\"].tolist()\n",
        "    n_test  = len(gold_qs)\n",
        "\n",
        "    pred_as, qsims = [], []\n",
        "    for q in gold_qs:\n",
        "        pred_a, _, qsim = ask_question(q, threshold=0.0)\n",
        "        pred_as.append(pred_a)\n",
        "        qsims.append(qsim)\n",
        "\n",
        "    print(\"[EVAL] Batch-encoding for semantic similarity ...\")\n",
        "    pred_vecs   = encode_passage(model, pred_as)\n",
        "    gold_vecs   = encode_passage(model, gold_as)\n",
        "    ans_cos_all = np.sum(pred_vecs * gold_vecs, axis=1)\n",
        "\n",
        "    exacts, tf1s, qcos1s, semhits = [], [], [], []\n",
        "    for i in range(n_test):\n",
        "        exacts.append(1.0 if normalize_text(pred_as[i]) == normalize_text(gold_as[i]) else 0.0)\n",
        "        tf1s.append(token_f1(pred_as[i], gold_as[i]))\n",
        "        qcos1s.append(float(qsims[i]) if np.isfinite(qsims[i]) else 0.0)\n",
        "        semhits.append(1.0 if float(ans_cos_all[i]) >= SEM_THR else 0.0)\n",
        "\n",
        "    results = {\n",
        "        \"Exact@1\":                   float(np.mean(exacts)),\n",
        "        \"TokenF1@1\":                 float(np.mean(tf1s)),\n",
        "        \"MeanCos@1(QSim)\":           float(np.mean(qcos1s)),\n",
        "        \"Semantic@1(ans_cos>=0.85)\": float(np.mean(semhits)),\n",
        "        \"BERTScoreF1@1\":             None,\n",
        "    }\n",
        "\n",
        "    if HAS_BERT_SCORE:\n",
        "        print(\"[EVAL] Computing BERTScore ...\")\n",
        "        _, _, F1 = bert_score_fn(pred_as, gold_as, lang=\"kk\", verbose=False)\n",
        "        results[\"BERTScoreF1@1\"] = float(F1.mean())\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(\"\\n=== Бағалау нәтижелері (TRAIN + EVAL: ТАЗА ДЕРЕКТЕР) ===\")\n",
        "    print(f\"  Exact@1:                    {results['Exact@1']:.6f}\")\n",
        "    print(f\"  TokenF1@1:                  {results['TokenF1@1']:.6f}\")\n",
        "    print(f\"  MeanCos@1(QSim):            {results['MeanCos@1(QSim)']:.6f}\")\n",
        "    print(f\"  Semantic@1(ans_cos>=0.85):  {results['Semantic@1(ans_cos>=0.85)']:.6f}\")\n",
        "    if results[\"BERTScoreF1@1\"] is None:\n",
        "        print(\"  BERTScoreF1@1:              (pip install bert-score to enable)\")\n",
        "    else:\n",
        "        print(f\"  BERTScoreF1@1:              {results['BERTScoreF1@1']:.6f}\")\n",
        "    print(f\"\\n  Elapsed: {elapsed:.2f}s | Test samples: {n_test}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# INTERACTIVE DIALOG\n",
        "# ================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Dialog  |  'eval' → бағалау  |  'exit' → шығу\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nСұрақ: \").strip()\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Stopped. 👋\")\n",
        "            break\n",
        "        if user_input.lower() == \"eval\":\n",
        "            evaluate_system()\n",
        "            continue\n",
        "        if not user_input:\n",
        "            continue\n",
        "        answer, _, qsim = ask_question(user_input, threshold=0.0)\n",
        "        print(f\"\\n  Жауап : {answer}\")\n",
        "        print(f\"  QSim  : {qsim:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dgeUHefgZlFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}