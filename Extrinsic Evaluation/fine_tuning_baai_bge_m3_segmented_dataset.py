# -*- coding: utf-8 -*-
"""Fine tuning BAAI/bge-m3  segmented dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wOm343ByEMMdWs0xrSmTk7oUViupS2ci
"""

!pip install bert-score
!pip install datasets
!pip install sentence-transformers datasets accelerate transformers wandb
!pip install sentence-transformers torch sklearn bert-score

!pip install peft -q

"""Segment BAAI/bge-m3 model"""

# @title
# ============================================================
# SEGMENTED QA ‚Üí Fine-tune (MORPH SEG ONLY) + Evaluation
# VERSION 4 ‚Äî LoRA/PEFT APPROACH (fixes persistent OOM on 15 GB GPU)
#
# ROOT CAUSE of OOM: e5-large (560M params) + Adam optimizer states
# (2x weights) + fp32 activations = ~15 GB before any batch data.
#
# SOLUTION: LoRA (Low-Rank Adaptation)
#   - Freeze all 560M base weights (no grad ‚Üí no optimizer state for them)
#   - Add tiny trainable adapters (~2-4M params) to attention layers
#   - VRAM drops from ~15 GB ‚Üí ~3-4 GB during training
#   - Quality loss vs full fine-tune: minimal for retrieval tasks
#
# MODEL: BAAI/bge-m3  (E5-large –æ—Ä–Ω—ã–Ω–∞)
#   - BGE-M3 –µ—Ä–µ–∫—à–µ–ª—ñ–≥—ñ: query/passage prefix “ö–ê–ñ–ï–¢ –ï–ú–ï–°
#     (E5-instruct-—Ç–µ–Ω –∞–π—ã—Ä–º–∞—à—ã–ª—ã“ì—ã ‚Äî –±–æ—Å prefix)
#   - LoRA target modules: "query" / "value" ‚Üí "query" / "value"
#     (XLM-R –Ω–µ–≥—ñ–∑—ñ–Ω–¥–µ–≥—ñ BGE-M3-—Ç–µ –∞—Ç–∞—É–ª–∞—Ä –±—ñ—Ä–¥–µ–π)
#   - encode_query / encode_passage: –µ–∫–µ—É—ñ –¥–µ prefix-—Å—ñ–∑
#
# Requires: pip install peft
#
# Metrics:
#   Exact@1 | TokenF1@1 | MeanCos@1(QSim)
#   Semantic@1(ans_cos>=0.85) | BERTScoreF1@1 (optional)
# ============================================================

import os
os.environ["WANDB_DISABLED"]              = "true"
os.environ["TOKENIZERS_PARALLELISM"]      = "false"
os.environ["TRANSFORMERS_NO_TORCHVISION"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"]     = "expandable_segments:True"

import pandas as pd
import numpy as np
import time, re, json, glob, random
import torch
from pathlib import Path
from collections import Counter

# ‚îÄ‚îÄ PEFT / LoRA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from peft import LoraConfig, get_peft_model, TaskType
    HAS_PEFT = True
except ImportError:
    HAS_PEFT = False
    print("‚ö†Ô∏è  peft not installed. Run:  pip install peft")
    print("    Falling back to frozen-backbone training (only pooler trained).")

from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from transformers import AutoModel

# BERTScore ‚Äî optional
try:
    from bert_score import score as bert_score_fn
    HAS_BERT_SCORE = True
except ImportError:
    bert_score_fn  = None
    HAS_BERT_SCORE = False


# ================================================================
# CONFIG
# ================================================================
DATA_PATH   = "kazakh_segmented_15000.json"

# ‚úÖ ”®–ó–ì–ï–†–Ü–° 1: –ú–æ–¥–µ–ª—å –∞—É—ã—Å—Ç—ã—Ä—ã–ª–¥—ã
MODEL_NAME  = "BAAI/bge-m3"              # –±“±—Ä—ã–Ω: intfloat/multilingual-e5-large-instruct

OUTPUT_PATH = "fine-tuned-kz-model-bge-m3"  # –±“±—Ä—ã–Ω: fine-tuned-kz-model-e5-large

TEST_SIZE    = 0.4
EPOCHS       = 3
WARMUP_STEPS = 100
BATCH_SIZE   = 16
SEM_THR      = 0.85
SEED         = 42
USE_FP16     = torch.cuda.is_available()

LORA_R       = 16
LORA_ALPHA   = 32
LORA_DROPOUT = 0.05
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 2: BGE-M3 XLM-R –Ω–µ–≥—ñ–∑—ñ–Ω–¥–µ ‚Äî layer –∞—Ç–∞—É–ª–∞—Ä—ã –±—ñ—Ä–¥–µ–π,
#    –±—ñ—Ä–∞“õ –Ω–∞“õ—Ç—ã –∞—Ç–∞—É–ª–∞—Ä–¥—ã –º–æ–¥–µ–ª—å –≥—Ä–∞—Ñ –∞—Ä“õ—ã–ª—ã —Ç–µ–∫—Å–µ—Ä–µ–º—ñ–∑ (—Ç”©–º–µ–Ω–¥–µ)
LORA_TARGETS = ["query", "value"]


# ================================================================
# REPRODUCIBILITY
# ================================================================
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(SEED)


# ================================================================
# FILE UTILITIES
# ================================================================
def find_data_path(p: str) -> str:
    if Path(p).exists():
        return p
    for c in [f"/content/{p}", f"/content/drive/MyDrive/{p}"]:
        if Path(c).exists():
            return c
    name = Path(p).name
    hits = glob.glob(f"**/{name}", recursive=True)
    if hits:
        return hits[0]
    near = glob.glob("**/*.json", recursive=True)
    raise FileNotFoundError(
        f"File not found: {p}\nPWD: {Path.cwd()}\n"
        f"Nearby .json files:\n" + "\n".join(near[:30])
    )


def _normalize_records(data: list) -> list:
    out = []
    for x in data:
        if not isinstance(x, dict):
            continue
        q = x.get("question") or x.get("instruction") or ""
        a = x.get("answer")   or x.get("response")    or ""
        q, a = str(q).strip(), str(a).strip()
        if q and a:
            out.append({"question": q, "answer": a})
    if not out:
        raise ValueError("No valid question/answer pairs found.")
    return out


def load_qa_records(path: str) -> list:
    text = Path(path).read_text(encoding="utf-8", errors="ignore").strip()
    if not text:
        raise ValueError(f"File is empty: {path}")

    if text[0] == "[":
        try:
            return _normalize_records(json.loads(text))
        except Exception:
            pass

    lines = [ln.strip().rstrip(",") for ln in text.splitlines() if ln.strip()]
    if lines and lines[0].startswith("{"):
        recs, ok = [], True
        for ln in lines:
            try:
                recs.append(json.loads(ln))
            except Exception:
                ok = False; break
        if ok and recs:
            return _normalize_records(recs)

    objs, buf, depth = [], [], 0
    in_str = esc = started = False
    for ch in text:
        if not started:
            if ch == "{":
                started, depth, buf = True, 1, ["{"]
            continue
        buf.append(ch)
        if in_str:
            if esc:          esc = False
            elif ch == "\\": esc = True
            elif ch == '"':  in_str = False
        else:
            if   ch == '"': in_str = True
            elif ch == "{": depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    try:
                        objs.append(json.loads("".join(buf)))
                    except Exception:
                        pass
                    buf, started = [], False

    if not objs:
        raise ValueError(f"Could not parse JSON. Preview:\n{text[:400]}")
    return _normalize_records(objs)


# ================================================================
# TEXT UTILITIES
# ================================================================
def clean_segmented_text(text: str) -> str:
    t = re.sub(r"@@\s*", "", str(text))
    t = re.sub(r"\s+", " ", t).strip()
    t = re.sub(r"\s+([,.!?;:])", r"\1", t)
    t = re.sub(r"\s*([-/])\s*", r"\1", t)
    return t


def normalize_text(t: str) -> str:
    return re.sub(r"\s+", " ", str(t).lower().strip())


KZ_PAT = re.compile(
    r"[a-zA-Z–∞-—è–ê-–Ø”ô“ì“õ“£”©“±“Ø“ª—ñ”ò“í“ö“¢”®“∞“Æ“∫–Ü0-9]+"
)

def tokens(text: str) -> list:
    return KZ_PAT.findall(normalize_text(text))


def token_f1(pred: str, gold: str) -> float:
    p, g = tokens(pred), tokens(gold)
    if not p and not g: return 1.0
    if not p or  not g: return 0.0
    pc, gc = Counter(p), Counter(g)
    inter  = sum((pc & gc).values())
    if inter == 0: return 0.0
    prec = inter / len(p)
    rec  = inter / len(g)
    return (2 * prec * rec) / (prec + rec + 1e-12)


# ================================================================
# BGE-M3 ENCODING
#
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 3: BGE-M3 ‚Äî E5-instruct-—Ç–µ–Ω –ù–ï–ì–Ü–ó–ì–Ü –ê–ô–´–†–ú–ê–®–´–õ–´“ö
#
#   E5-instruct:
#     query   ‚Üí "Instruct: ...\nQuery: {text}"   (–º—ñ–Ω–¥–µ—Ç—Ç—ñ prefix)
#     passage ‚Üí —Ç—ñ–∫–µ–ª–µ–π –º”ô—Ç—ñ–Ω
#
#   BGE-M3:
#     query   ‚Üí —Ç—ñ–∫–µ–ª–µ–π –º”ô—Ç—ñ–Ω  (prefix “ö–ê–ñ–ï–¢ –ï–ú–ï–°)
#     passage ‚Üí —Ç—ñ–∫–µ–ª–µ–π –º”ô—Ç—ñ–Ω  (prefix “ö–ê–ñ–ï–¢ –ï–ú–ï–°)
#
#   BGE-M3 —Ä–µ—Å–º–∏ –Ω“±—Å“õ–∞—É–ª—ã“ì—ã (BAAI):
#     "No instruction is needed for either queries or passages."
#
#   encode_query == encode_passage (–µ–∫–µ—É—ñ –¥–µ prefix-—Å—ñ–∑)
#   –ë—ñ—Ä–∞“õ —Ñ—É–Ω–∫—Ü–∏—è–ª–∞—Ä–¥—ã –±”©–ª—ñ–ø —Å–∞“õ—Ç–∞–π–º—ã–∑ ‚Äî –±–æ–ª–∞—à–∞“õ—Ç–∞ “õ–∞–∂–µ—Ç –±–æ–ª—Å–∞
#   –æ“£–∞–π ”©–∑–≥–µ—Ä—Ç—É–≥–µ –º“Ø–º–∫—ñ–Ω–¥—ñ–∫ –±–µ—Ä–µ–¥—ñ.
# ================================================================

def _nan_guard(vecs: np.ndarray, label: str) -> np.ndarray:
    bad = ~np.isfinite(vecs).all(axis=1)
    if bad.any():
        print(f"  ‚ö† {bad.sum()} {label} vectors had NaN/Inf ‚Üí zeroed.")
        vecs[bad] = 0.0
    return vecs


def encode_query(mdl: SentenceTransformer, texts: list) -> np.ndarray:
    # ‚úÖ BGE-M3: prefix –∂–æ“õ ‚Äî –º”ô—Ç—ñ–Ω–¥—ñ —Ç—ñ–∫–µ–ª–µ–π –±–µ—Ä–µ–º—ñ–∑
    vecs = mdl.encode(
        texts,
        convert_to_numpy=True,
        normalize_embeddings=True,
        batch_size=32,
        show_progress_bar=False,
    )
    return _nan_guard(vecs, "query")


def encode_passage(mdl: SentenceTransformer, texts: list) -> np.ndarray:
    # ‚úÖ BGE-M3: passage –¥–∞ prefix-—Å—ñ–∑
    vecs = mdl.encode(
        texts,
        convert_to_numpy=True,
        normalize_embeddings=True,
        batch_size=32,
        show_progress_bar=False,
    )
    return _nan_guard(vecs, "passage")


# ================================================================
# LOAD DATA
# ================================================================
data_path      = find_data_path(DATA_PATH)
rows           = load_qa_records(data_path)
segmented_data = pd.DataFrame(rows)
print(f"[LOADED] rows={len(segmented_data)} | path={data_path}")

clean_data = segmented_data.copy()
clean_data["question"] = clean_data["question"].apply(clean_segmented_text)
clean_data["answer"]   = clean_data["answer"].apply(clean_segmented_text)


# ================================================================
# TRAIN / TEST SPLIT
# ================================================================
idx = np.arange(len(segmented_data))
train_idx, test_idx = train_test_split(
    idx, test_size=TEST_SIZE, random_state=SEED, shuffle=True
)
seg_train = segmented_data.iloc[train_idx].reset_index(drop=True)
cln_train = clean_data.iloc[train_idx].reset_index(drop=True)
cln_test  = clean_data.iloc[test_idx].reset_index(drop=True)
print(f"[SPLIT] train={len(cln_train)} | test={len(cln_test)}")


# ================================================================
# LOAD BASE MODEL
# ================================================================
print(f"\n[MODEL] Loading {MODEL_NAME} ...")
model = SentenceTransformer(MODEL_NAME)


# ================================================================
# BGE-M3 LoRA TARGET MODULES –ê–í–¢–û–ú–ê–¢–¢–´ –ê–ù–´“ö–¢–ê–£
#
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 4: BGE-M3 attention layer –∞—Ç–∞—É–ª–∞—Ä—ã–Ω –∞–≤—Ç–æ–º–∞—Ç—Ç—ã —Ç–µ–∫—Å–µ—Ä—É.
#    –ï–≥–µ—Ä "query"/"value" –∞—Ç–∞—É–ª–∞—Ä—ã –∂–æ“õ –±–æ–ª—Å–∞ (–∫–µ–π –Ω“±—Å“õ–∞–ª–∞—Ä–¥–∞
#    "q_proj"/"v_proj" –¥–µ–ø –∞—Ç–∞–ª—É—ã –º“Ø–º–∫—ñ–Ω), –¥“±—Ä—ã—Å –∞—Ç–∞—É–¥—ã —Ç–∞–±–∞–º—ã–∑.
# ================================================================
def detect_lora_targets(st_model: SentenceTransformer) -> list:
    """BGE-M3 backbone-–Ω—ñ“£ –Ω–∞“õ—Ç—ã attention projection –∞—Ç–∞—É–ª–∞—Ä—ã–Ω —Ç–∞–±—É."""
    backbone    = st_model[0].auto_model
    linear_names = set()
    for name, module in backbone.named_modules():
        if isinstance(module, torch.nn.Linear):
            # —Ç–µ–∫ —Å–æ“£“ì—ã –±”©–ª—ñ–∫—Ç—ñ –∞–ª–∞–º—ã–∑: encoder.layer.0.attention.self.query ‚Üí query
            leaf = name.split(".")[-1]
            linear_names.add(leaf)

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç”ô—Ä—Ç—ñ–±—ñ–º–µ–Ω —Ç–µ–∫—Å–µ—Ä–µ–º—ñ–∑
    candidates = [
        ["query", "value"],       # XLM-R / BERT —Å—Ç–∞–Ω–¥–∞—Ä—Ç
        ["q_proj", "v_proj"],     # LLaMA / Mistral —Å—Ç–∏–ª—å
        ["q", "v"],               # “õ—ã—Å“õ–∞ –Ω“±—Å“õ–∞
    ]
    for pair in candidates:
        if all(t in linear_names for t in pair):
            print(f"[LoRA] Detected target modules: {pair}")
            return pair

    # –ï—à—Ç–µ“£–µ —Ç–∞–±—ã–ª–º–∞—Å–∞ ‚Äî –±–∞—Ä–ª—ã“õ linear layer (“õ–∞—É—ñ–ø—Å—ñ–∑ fallback)
    fallback = list(linear_names)[:4]
    print(f"[LoRA] Standard targets not found. Using fallback: {fallback}")
    return fallback


# ================================================================
# APPLY LoRA TO THE BACKBONE
# ================================================================
def apply_lora(st_model: SentenceTransformer) -> SentenceTransformer:
    backbone = st_model[0].auto_model

    # ‚úÖ –ê–≤—Ç–æ–º–∞—Ç—Ç—ã target –∞–Ω—ã“õ—Ç–∞—É
    detected_targets = detect_lora_targets(st_model)

    lora_cfg = LoraConfig(
        task_type=TaskType.FEATURE_EXTRACTION,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=detected_targets,   # ‚úÖ hardcode –µ–º–µ—Å, –∞–≤—Ç–æ
        bias="none",
    )

    peft_backbone = get_peft_model(backbone, lora_cfg)

    total  = sum(p.numel() for p in peft_backbone.parameters())
    train_ = sum(p.numel() for p in peft_backbone.parameters() if p.requires_grad)
    print(f"[LoRA] Total params : {total/1e6:.1f}M")
    print(f"[LoRA] Trainable    : {train_/1e6:.2f}M  "
          f"({100*train_/total:.2f}% of total)")

    st_model[0].auto_model = peft_backbone
    return st_model


def freeze_backbone(st_model: SentenceTransformer) -> SentenceTransformer:
    for name, param in st_model[0].auto_model.named_parameters():
        param.requires_grad = False
    total  = sum(p.numel() for p in st_model.parameters())
    train_ = sum(p.numel() for p in st_model.parameters() if p.requires_grad)
    print(f"[FROZEN] Trainable params: {train_/1e6:.2f}M / {total/1e6:.1f}M total")
    return st_model


if HAS_PEFT:
    model = apply_lora(model)
    print("[LoRA] Adapters applied. Backbone frozen.")
else:
    model = freeze_backbone(model)
    print("[FROZEN] Backbone frozen (install peft for better results).")

if torch.cuda.is_available():
    alloc = torch.cuda.memory_allocated() / 1e9
    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"[GPU] After model prep: {alloc:.2f} GB / {total_vram:.1f} GB allocated")


# ================================================================
# BUILD TRAINING EXAMPLES  (with hard negatives)
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 5: BGE-M3 “Ø—à—ñ–Ω e5_wrap_query() –ñ–û“ö ‚Äî
#    seg_q —Ç—ñ–∫–µ–ª–µ–π InputExample-–≥–µ –±–µ—Ä–µ–º—ñ–∑ (prefix-—Å—ñ–∑)
# ================================================================
print("\n[TRAIN] Building training examples ...")
train_examples  = []
n_train         = len(seg_train)
all_seg_answers = seg_train["answer"].tolist()

for i in range(n_train):
    seg_q = str(seg_train.loc[i, "question"]).strip()
    seg_a = str(seg_train.loc[i, "answer"]).strip()
    if not seg_q or not seg_a:
        continue
    neg_idx = random.choice([k for k in range(n_train) if k != i])
    neg_a   = str(all_seg_answers[neg_idx]).strip()
    # ‚úÖ BGE-M3: query-“ì–∞ prefix “õ–æ—Å–ø–∞–π–º—ã–∑
    train_examples.append(
        InputExample(texts=[seg_q, seg_a, neg_a])
    )

if len(train_examples) < 2:
    raise ValueError("Not enough training pairs.")

print(f"[TRAIN] {len(train_examples)} examples | batch_size={BATCH_SIZE}")

train_dataloader = DataLoader(
    train_examples,
    shuffle=True,
    batch_size=BATCH_SIZE,
    drop_last=True,
)

try:
    train_loss = losses.CachedMultipleNegativesRankingLoss(
        model, mini_batch_size=8
    )
    print("[LOSS] CachedMultipleNegativesRankingLoss")
except AttributeError:
    train_loss = losses.MultipleNegativesRankingLoss(model)
    print("[LOSS] MultipleNegativesRankingLoss")


# ================================================================
# CLEAR VRAM BEFORE TRAINING
# ================================================================
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    free = (torch.cuda.get_device_properties(0).total_memory
            - torch.cuda.memory_allocated()) / 1e9
    print(f"\n[GPU] Free VRAM before fit: {free:.2f} GB")


# ================================================================
# FINE-TUNING
# ================================================================
print(f"\n[FINE-TUNE] Epochs={EPOCHS} | warmup={WARMUP_STEPS} | fp16={USE_FP16}")

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=EPOCHS,
    warmup_steps=WARMUP_STEPS,
    show_progress_bar=True,
    use_amp=USE_FP16,
    optimizer_params={"lr": 2e-4},
)


# ================================================================
# MERGE LoRA WEIGHTS & SAVE
# ================================================================
if HAS_PEFT:
    print("\n[LoRA] Merging adapter weights into base model ...")
    try:
        model[0].auto_model = model[0].auto_model.merge_and_unload()
        print("[LoRA] Merge complete.")
    except Exception as e:
        print(f"[LoRA] Merge failed ({e}) ‚Äî saving with adapter weights.")

model.save(OUTPUT_PATH)
print(f"[SAVED] {OUTPUT_PATH}")

if torch.cuda.is_available():
    torch.cuda.empty_cache()

model = SentenceTransformer(OUTPUT_PATH)
print("[LOADED] Model reloaded from disk.")


# ================================================================
# BUILD KB INDEX
# ================================================================
train_questions = cln_train["question"].tolist()
train_answers   = cln_train["answer"].tolist()

print(f"\n[KB] Indexing {len(train_questions)} training questions ...")
train_q_embeds = encode_query(model, train_questions)
print(f"[KB] Index shape: {train_q_embeds.shape}")


# ================================================================
# RETRIEVAL
# ================================================================
def ask_question(question: str, threshold: float = 0.0):
    q_clean    = clean_segmented_text(question)
    q_vec      = encode_query(model, [q_clean])
    sims       = q_vec @ train_q_embeds.T
    best_idx   = int(np.argmax(sims[0]))
    best_score = float(sims[0][best_idx])

    if not np.isfinite(best_score):
        return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –Ω–∞“õ—Ç—ã –∂–∞—É–∞–ø —Ç–∞–±—ã–ª–º–∞–¥—ã.", -1, 0.0
    if best_score < threshold:
        return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –Ω–∞“õ—Ç—ã –∂–∞—É–∞–ø —Ç–∞–±—ã–ª–º–∞–¥—ã.", -1, best_score
    return train_answers[best_idx], best_idx, best_score


# ================================================================
# EVALUATION (batch-encoded)
# ================================================================
def evaluate_system() -> dict:
    print("\n[EVAL] Running ...")
    t0 = time.time()

    gold_qs = cln_test["question"].tolist()
    gold_as = cln_test["answer"].tolist()
    n_test  = len(gold_qs)

    pred_as, qsims = [], []
    for q in gold_qs:
        pred_a, _, qsim = ask_question(q, threshold=0.0)
        pred_as.append(pred_a)
        qsims.append(qsim)

    print("[EVAL] Batch-encoding for semantic similarity ...")
    pred_vecs   = encode_passage(model, pred_as)
    gold_vecs   = encode_passage(model, gold_as)
    ans_cos_all = np.sum(pred_vecs * gold_vecs, axis=1)

    exacts, tf1s, qcos1s, semhits = [], [], [], []
    for i in range(n_test):
        exacts.append(1.0 if normalize_text(pred_as[i]) == normalize_text(gold_as[i]) else 0.0)
        tf1s.append(token_f1(pred_as[i], gold_as[i]))
        qcos1s.append(float(qsims[i]) if np.isfinite(qsims[i]) else 0.0)
        semhits.append(1.0 if float(ans_cos_all[i]) >= SEM_THR else 0.0)

    results = {
        "Exact@1":                   float(np.mean(exacts)),
        "TokenF1@1":                 float(np.mean(tf1s)),
        "MeanCos@1(QSim)":           float(np.mean(qcos1s)),
        "Semantic@1(ans_cos>=0.85)": float(np.mean(semhits)),
        "BERTScoreF1@1":             None,
    }

    if HAS_BERT_SCORE:
        print("[EVAL] Computing BERTScore ...")
        _, _, F1 = bert_score_fn(pred_as, gold_as, lang="kk", verbose=False)
        results["BERTScoreF1@1"] = float(F1.mean())

    elapsed = time.time() - t0
    print("\n=== Evaluation Results ===")
    print(f"  Exact@1:                    {results['Exact@1']:.6f}")
    print(f"  TokenF1@1:                  {results['TokenF1@1']:.6f}")
    print(f"  MeanCos@1(QSim):            {results['MeanCos@1(QSim)']:.6f}")
    print(f"  Semantic@1(ans_cos>=0.85):  {results['Semantic@1(ans_cos>=0.85)']:.6f}")
    if results["BERTScoreF1@1"] is None:
        print("  BERTScoreF1@1:              (pip install bert-score to enable)")
    else:
        print(f"  BERTScoreF1@1:              {results['BERTScoreF1@1']:.6f}")
    print(f"\n  Elapsed: {elapsed:.2f}s | Test samples: {n_test}")
    return results


# ================================================================
# INTERACTIVE DIALOG
# ================================================================
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("Dialog  |  'eval' ‚Üí evaluation  |  'exit' ‚Üí quit")
    print("=" * 60)

    while True:
        user_input = input("\n–°“±—Ä–∞“õ: ").strip()
        if user_input.lower() == "exit":
            print("Stopped. üëã")
            break
        if user_input.lower() == "eval":
            evaluate_system()
            continue
        if not user_input:
            continue
        answer, _, qsim = ask_question(user_input, threshold=0.0)
        print(f"\n  –ñ–∞—É–∞–ø : {answer}")
        print(f"  QSim  : {qsim:.4f}")