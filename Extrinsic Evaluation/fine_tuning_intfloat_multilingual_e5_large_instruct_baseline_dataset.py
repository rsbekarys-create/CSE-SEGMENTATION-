# -*- coding: utf-8 -*-
"""Fine tuning intfloat/multilingual-e5-large-instruct baseline dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wOm343ByEMMdWs0xrSmTk7oUViupS2ci
"""

!pip install bert-score
!pip install datasets
!pip install sentence-transformers datasets accelerate transformers wandb
!pip install sentence-transformers torch sklearn bert-score

!pip install peft -q

"""baseline intfloat/multilingual-e5-large-instruct"""

# @title
# ============================================================
# CLEAN QA ‚Üí Fine-tune + Evaluation
# VERSION 5 ‚Äî –¢–ê–ó–ê –î–ê–¢–ê–°–ï–¢ (—Å–µ–≥–º–µ–Ω—Ç—Ç–µ–ª–º–µ–≥–µ–Ω)
#
# ”®–ó–ì–ï–†–Ü–°–¢–ï–† (v4 ‚Üí v5):
#   - DATA_PATH  ‚Üí kazakh_qa_clean_15000.json  (—Ç–∞–∑–∞, @@-—Å—ñ–∑)
#   - seg_train  –∂–æ–π—ã–ª–¥—ã ‚Üí —Ç–µ–∫ cln_train “õ–æ–ª–¥–∞–Ω—ã–ª–∞–¥—ã
#   - clean_segmented_text() —Ç–µ–∫ “õ–∞—É—ñ–ø—Å—ñ–∑–¥—ñ–∫ “Ø—à—ñ–Ω —Å–∞“õ—Ç–∞–ª–¥—ã
#     (—Ç–∞–∑–∞ –¥–µ—Ä–µ–∫—Ç–µ @@ –∂–æ“õ –±–æ–ª—Å–∞ –¥–∞ –∑–∏—è–Ω –∂–æ“õ)
#   - train_examples: e5_wrap_query(cln_q) + cln_a  (—Ç–∞–∑–∞ –º”ô—Ç—ñ–Ω)
#   - –ë–∞—Ä–ª—ã“õ –±–∞—Å“õ–∞ –ª–æ–≥–∏–∫–∞ v4-–ø–µ–Ω –ë–Ü–†–î–ï–ô
#
# Metrics:
#   Exact@1 | TokenF1@1 | MeanCos@1(QSim)
#   Semantic@1(ans_cos>=0.85) | BERTScoreF1@1 (optional)
#
# Requires: pip install peft
# ============================================================

import os
os.environ["WANDB_DISABLED"]              = "true"
os.environ["TOKENIZERS_PARALLELISM"]      = "false"
os.environ["TRANSFORMERS_NO_TORCHVISION"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"]     = "expandable_segments:True"

import pandas as pd
import numpy as np
import time, re, json, glob, random
import torch
from pathlib import Path
from collections import Counter

# ‚îÄ‚îÄ PEFT / LoRA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from peft import LoraConfig, get_peft_model, TaskType
    HAS_PEFT = True
except ImportError:
    HAS_PEFT = False
    print("‚ö†Ô∏è  peft not installed. Run:  pip install peft")
    print("    Falling back to frozen-backbone training.")

from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

# BERTScore ‚Äî optional
try:
    from bert_score import score as bert_score_fn
    HAS_BERT_SCORE = True
except ImportError:
    bert_score_fn  = None
    HAS_BERT_SCORE = False


# ================================================================
# CONFIG
# ================================================================
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 1: –¢–∞–∑–∞ (—Å–µ–≥–º–µ–Ω—Ç—Ç–µ–ª–º–µ–≥–µ–Ω) –¥–∞—Ç–∞—Å–µ—Ç —Ñ–∞–π–ª—ã
DATA_PATH   = "baseline_15000.json"   # <-- –±“±—Ä—ã–Ω: kazakh_segmented_15000.json
MODEL_NAME  = "intfloat/multilingual-e5-large-instruct"
OUTPUT_PATH = "fine-tuned-kz-model-e5-large"

TEST_SIZE    = 0.4
EPOCHS       = 3
WARMUP_STEPS = 100
BATCH_SIZE   = 16
SEM_THR      = 0.85
SEED         = 42
USE_FP16     = torch.cuda.is_available()

LORA_R       = 16
LORA_ALPHA   = 32
LORA_DROPOUT = 0.05
LORA_TARGETS = ["query", "value"]


# ================================================================
# REPRODUCIBILITY
# ================================================================
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(SEED)


# ================================================================
# FILE UTILITIES
# ================================================================
def find_data_path(p: str) -> str:
    if Path(p).exists():
        return p
    for c in [f"/content/{p}", f"/content/drive/MyDrive/{p}"]:
        if Path(c).exists():
            return c
    name = Path(p).name
    hits = glob.glob(f"**/{name}", recursive=True)
    if hits:
        return hits[0]
    near = glob.glob("**/*.json", recursive=True)
    raise FileNotFoundError(
        f"File not found: {p}\nPWD: {Path.cwd()}\n"
        f"Nearby .json files:\n" + "\n".join(near[:30])
    )


def _normalize_records(data: list) -> list:
    out = []
    for x in data:
        if not isinstance(x, dict):
            continue
        q = x.get("question") or x.get("instruction") or ""
        a = x.get("answer")   or x.get("response")    or ""
        q, a = str(q).strip(), str(a).strip()
        if q and a:
            out.append({"question": q, "answer": a})
    if not out:
        raise ValueError("No valid question/answer pairs found.")
    return out


def load_qa_records(path: str) -> list:
    text = Path(path).read_text(encoding="utf-8", errors="ignore").strip()
    if not text:
        raise ValueError(f"File is empty: {path}")

    if text[0] == "[":
        try:
            return _normalize_records(json.loads(text))
        except Exception:
            pass

    lines = [ln.strip().rstrip(",") for ln in text.splitlines() if ln.strip()]
    if lines and lines[0].startswith("{"):
        recs, ok = [], True
        for ln in lines:
            try:
                recs.append(json.loads(ln))
            except Exception:
                ok = False; break
        if ok and recs:
            return _normalize_records(recs)

    objs, buf, depth = [], [], 0
    in_str = esc = started = False
    for ch in text:
        if not started:
            if ch == "{":
                started, depth, buf = True, 1, ["{"]
            continue
        buf.append(ch)
        if in_str:
            if esc:          esc = False
            elif ch == "\\": esc = True
            elif ch == '"':  in_str = False
        else:
            if   ch == '"': in_str = True
            elif ch == "{": depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    try:
                        objs.append(json.loads("".join(buf)))
                    except Exception:
                        pass
                    buf, started = [], False

    if not objs:
        raise ValueError(f"Could not parse JSON. Preview:\n{text[:400]}")
    return _normalize_records(objs)


# ================================================================
# TEXT UTILITIES
# ================================================================
def clean_segmented_text(text: str) -> str:
    # ‚úÖ –¢–∞–∑–∞ –¥–µ—Ä–µ–∫—Ç–µ @@ –±–æ–ª–º–∞–π–¥—ã, –±—ñ—Ä–∞“õ “õ–∞—É—ñ–ø—Å—ñ–∑–¥—ñ–∫ “Ø—à—ñ–Ω —Å–∞“õ—Ç–∞–π–º—ã–∑
    t = re.sub(r"@@\s*", "", str(text))
    t = re.sub(r"\s+", " ", t).strip()
    t = re.sub(r"\s+([,.!?;:])", r"\1", t)
    t = re.sub(r"\s*([-/])\s*", r"\1", t)
    return t


def normalize_text(t: str) -> str:
    return re.sub(r"\s+", " ", str(t).lower().strip())


KZ_PAT = re.compile(
    r"[a-zA-Z–∞-—è–ê-–Ø”ô“ì“õ“£”©“±“Ø“ª—ñ”ò“í“ö“¢”®“∞“Æ“∫–Ü0-9]+"
)

def tokens(text: str) -> list:
    return KZ_PAT.findall(normalize_text(text))


def token_f1(pred: str, gold: str) -> float:
    p, g = tokens(pred), tokens(gold)
    if not p and not g: return 1.0
    if not p or  not g: return 0.0
    pc, gc = Counter(p), Counter(g)
    inter  = sum((pc & gc).values())
    if inter == 0: return 0.0
    prec = inter / len(p)
    rec  = inter / len(g)
    return (2 * prec * rec) / (prec + rec + 1e-12)


# ================================================================
# E5-INSTRUCT ENCODING
# ================================================================
E5_QUERY_INSTRUCTION = (
    "Instruct: Given a question in Kazakh, "
    "retrieve the most relevant answer.\nQuery: "
)

def e5_wrap_query(text: str) -> str:
    return E5_QUERY_INSTRUCTION + text

def _nan_guard(vecs: np.ndarray, label: str) -> np.ndarray:
    bad = ~np.isfinite(vecs).all(axis=1)
    if bad.any():
        print(f"  ‚ö† {bad.sum()} {label} vectors had NaN/Inf ‚Üí zeroed.")
        vecs[bad] = 0.0
    return vecs

def encode_query(mdl: SentenceTransformer, texts: list) -> np.ndarray:
    vecs = mdl.encode(
        [e5_wrap_query(t) for t in texts],
        convert_to_numpy=True,
        normalize_embeddings=True,
        batch_size=32,
        show_progress_bar=False,
    )
    return _nan_guard(vecs, "query")

def encode_passage(mdl: SentenceTransformer, texts: list) -> np.ndarray:
    vecs = mdl.encode(
        texts,
        convert_to_numpy=True,
        normalize_embeddings=True,
        batch_size=32,
        show_progress_bar=False,
    )
    return _nan_guard(vecs, "passage")


# ================================================================
# LOAD DATA
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 2: –¢–µ–∫ —Ç–∞–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç –∂“Ø–∫—Ç–µ–π–º—ñ–∑.
#    seg_train –∂–æ“õ ‚Äî –±”ô—Ä—ñ cln_train/cln_test –∞—Ä“õ—ã–ª—ã –∂“Ø—Ä–µ–¥—ñ.
# ================================================================
data_path = find_data_path(DATA_PATH)
rows      = load_qa_records(data_path)

# ‚úÖ –¢—ñ–∫–µ–ª–µ–π —Ç–∞–∑–∞ DataFrame ‚Äî —Å–µ–≥–º–µ–Ω—Ç—Ç–µ–ª–≥–µ–Ω –ø–∞—Ä–∞–ª–ª–µ–ª—å –Ω“±—Å“õ–∞ –∂–æ“õ
clean_data = pd.DataFrame(rows)
clean_data["question"] = clean_data["question"].apply(clean_segmented_text)
clean_data["answer"]   = clean_data["answer"].apply(clean_segmented_text)

print(f"[LOADED] rows={len(clean_data)} | path={data_path}")
print(f"[DATA]   “Æ–ª–≥—ñ —Å“±—Ä–∞“õ : {clean_data['question'].iloc[0][:80]}")
print(f"[DATA]   “Æ–ª–≥—ñ –∂–∞—É–∞–ø : {clean_data['answer'].iloc[0][:80]}")


# ================================================================
# TRAIN / TEST SPLIT
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 3: seg_train –∂–æ“õ, —Ç–µ–∫ cln_train / cln_test
# ================================================================
idx = np.arange(len(clean_data))
train_idx, test_idx = train_test_split(
    idx, test_size=TEST_SIZE, random_state=SEED, shuffle=True
)

cln_train = clean_data.iloc[train_idx].reset_index(drop=True)
cln_test  = clean_data.iloc[test_idx].reset_index(drop=True)

print(f"[SPLIT] train={len(cln_train)} | test={len(cln_test)}")


# ================================================================
# LOAD BASE MODEL
# ================================================================
print(f"\n[MODEL] Loading {MODEL_NAME} ...")
model = SentenceTransformer(MODEL_NAME)


# ================================================================
# APPLY LoRA TO THE BACKBONE
# ================================================================
def apply_lora(st_model: SentenceTransformer) -> SentenceTransformer:
    backbone = st_model[0].auto_model

    lora_cfg = LoraConfig(
        task_type=TaskType.FEATURE_EXTRACTION,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=LORA_TARGETS,
        bias="none",
    )

    peft_backbone = get_peft_model(backbone, lora_cfg)

    total  = sum(p.numel() for p in peft_backbone.parameters())
    train_ = sum(p.numel() for p in peft_backbone.parameters() if p.requires_grad)
    print(f"[LoRA] Total params : {total/1e6:.1f}M")
    print(f"[LoRA] Trainable    : {train_/1e6:.2f}M  ({100*train_/total:.2f}% of total)")

    st_model[0].auto_model = peft_backbone
    return st_model


def freeze_backbone(st_model: SentenceTransformer) -> SentenceTransformer:
    for param in st_model[0].auto_model.parameters():
        param.requires_grad = False
    total  = sum(p.numel() for p in st_model.parameters())
    train_ = sum(p.numel() for p in st_model.parameters() if p.requires_grad)
    print(f"[FROZEN] Trainable: {train_/1e6:.2f}M / {total/1e6:.1f}M total")
    return st_model


if HAS_PEFT:
    model = apply_lora(model)
    print("[LoRA] Adapters applied. Backbone frozen.")
else:
    model = freeze_backbone(model)
    print("[FROZEN] Backbone frozen (install peft for better results).")

if torch.cuda.is_available():
    alloc      = torch.cuda.memory_allocated() / 1e9
    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"[GPU] After model prep: {alloc:.2f} GB / {total_vram:.1f} GB")


# ================================================================
# BUILD TRAINING EXAMPLES
# ‚úÖ ”®–ó–ì–ï–†–Ü–° 4: cln_train “õ–æ–ª–¥–∞–Ω–∞–º—ã–∑ (–±“±—Ä—ã–Ω seg_train –±–æ–ª–∞—Ç—ã–Ω)
#    –Ø“ì–Ω–∏ fine-tune –¥–∞, KB –¥–∞, eval –¥–∞ ‚Äî –±”ô—Ä—ñ —Ç–∞–∑–∞ –º”ô—Ç—ñ–Ω–º–µ–Ω
# ================================================================
print("\n[TRAIN] Building training examples ...")
train_examples   = []
n_train          = len(cln_train)
all_cln_answers  = cln_train["answer"].tolist()

for i in range(n_train):
    cln_q = str(cln_train.loc[i, "question"]).strip()
    cln_a = str(cln_train.loc[i, "answer"]).strip()
    if not cln_q or not cln_a:
        continue
    # Hard negative: –±–∞—Å“õ–∞ –∂–æ–ª–¥—ã“£ –∂–∞—É–∞–±—ã
    neg_idx = random.choice([k for k in range(n_train) if k != i])
    neg_a   = str(all_cln_answers[neg_idx]).strip()
    # [anchor_query, positive_passage, hard_negative_passage]
    train_examples.append(
        InputExample(texts=[e5_wrap_query(cln_q), cln_a, neg_a])
    )

if len(train_examples) < 2:
    raise ValueError("Not enough training pairs.")

print(f"[TRAIN] {len(train_examples)} examples | batch_size={BATCH_SIZE}")

train_dataloader = DataLoader(
    train_examples,
    shuffle=True,
    batch_size=BATCH_SIZE,
    drop_last=True,
)

try:
    train_loss = losses.CachedMultipleNegativesRankingLoss(
        model, mini_batch_size=8
    )
    print("[LOSS] CachedMultipleNegativesRankingLoss")
except AttributeError:
    train_loss = losses.MultipleNegativesRankingLoss(model)
    print("[LOSS] MultipleNegativesRankingLoss")


# ================================================================
# CLEAR VRAM BEFORE TRAINING
# ================================================================
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    free = (torch.cuda.get_device_properties(0).total_memory
            - torch.cuda.memory_allocated()) / 1e9
    print(f"\n[GPU] Free VRAM before fit: {free:.2f} GB")


# ================================================================
# FINE-TUNING
# ================================================================
print(f"\n[FINE-TUNE] Epochs={EPOCHS} | warmup={WARMUP_STEPS} | fp16={USE_FP16}")

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=EPOCHS,
    warmup_steps=WARMUP_STEPS,
    show_progress_bar=True,
    use_amp=USE_FP16,
    optimizer_params={"lr": 2e-4},
)


# ================================================================
# MERGE LoRA WEIGHTS & SAVE
# ================================================================
if HAS_PEFT:
    print("\n[LoRA] Merging adapter weights into base model ...")
    try:
        model[0].auto_model = model[0].auto_model.merge_and_unload()
        print("[LoRA] Merge complete.")
    except Exception as e:
        print(f"[LoRA] Merge failed ({e}) ‚Äî saving with adapter weights.")

model.save(OUTPUT_PATH)
print(f"[SAVED] {OUTPUT_PATH}")

if torch.cuda.is_available():
    torch.cuda.empty_cache()

model = SentenceTransformer(OUTPUT_PATH)
print("[LOADED] Model reloaded from disk.")


# ================================================================
# BUILD KB INDEX
# ‚úÖ encode_query (instruct prefix) ‚Äî retrieval-–º–µ–Ω —Å–∏–º–º–µ—Ç—Ä–∏—è–ª—ã
# ================================================================
train_questions = cln_train["question"].tolist()
train_answers   = cln_train["answer"].tolist()

print(f"\n[KB] Indexing {len(train_questions)} training questions ...")
train_q_embeds = encode_query(model, train_questions)
print(f"[KB] Index shape: {train_q_embeds.shape}")


# ================================================================
# RETRIEVAL
# ================================================================
def ask_question(question: str, threshold: float = 0.0):
    q_clean    = clean_segmented_text(question)
    q_vec      = encode_query(model, [q_clean])
    sims       = q_vec @ train_q_embeds.T
    best_idx   = int(np.argmax(sims[0]))
    best_score = float(sims[0][best_idx])

    if not np.isfinite(best_score):
        return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –Ω–∞“õ—Ç—ã –∂–∞—É–∞–ø —Ç–∞–±—ã–ª–º–∞–¥—ã.", -1, 0.0
    if best_score < threshold:
        return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –Ω–∞“õ—Ç—ã –∂–∞—É–∞–ø —Ç–∞–±—ã–ª–º–∞–¥—ã.", -1, best_score
    return train_answers[best_idx], best_idx, best_score


# ================================================================
# EVALUATION
# ================================================================
def evaluate_system() -> dict:
    print("\n[EVAL] Running ...")
    t0 = time.time()

    gold_qs = cln_test["question"].tolist()
    gold_as = cln_test["answer"].tolist()
    n_test  = len(gold_qs)

    pred_as, qsims = [], []
    for q in gold_qs:
        pred_a, _, qsim = ask_question(q, threshold=0.0)
        pred_as.append(pred_a)
        qsims.append(qsim)

    print("[EVAL] Batch-encoding for semantic similarity ...")
    pred_vecs   = encode_passage(model, pred_as)
    gold_vecs   = encode_passage(model, gold_as)
    ans_cos_all = np.sum(pred_vecs * gold_vecs, axis=1)

    exacts, tf1s, qcos1s, semhits = [], [], [], []
    for i in range(n_test):
        exacts.append(1.0 if normalize_text(pred_as[i]) == normalize_text(gold_as[i]) else 0.0)
        tf1s.append(token_f1(pred_as[i], gold_as[i]))
        qcos1s.append(float(qsims[i]) if np.isfinite(qsims[i]) else 0.0)
        semhits.append(1.0 if float(ans_cos_all[i]) >= SEM_THR else 0.0)

    results = {
        "Exact@1":                   float(np.mean(exacts)),
        "TokenF1@1":                 float(np.mean(tf1s)),
        "MeanCos@1(QSim)":           float(np.mean(qcos1s)),
        "Semantic@1(ans_cos>=0.85)": float(np.mean(semhits)),
        "BERTScoreF1@1":             None,
    }

    if HAS_BERT_SCORE:
        print("[EVAL] Computing BERTScore ...")
        _, _, F1 = bert_score_fn(pred_as, gold_as, lang="kk", verbose=False)
        results["BERTScoreF1@1"] = float(F1.mean())

    elapsed = time.time() - t0
    print("\n=== –ë–∞“ì–∞–ª–∞—É –Ω”ô—Ç–∏–∂–µ–ª–µ—Ä—ñ (TRAIN + EVAL: –¢–ê–ó–ê –î–ï–†–ï–ö–¢–ï–†) ===")
    print(f"  Exact@1:                    {results['Exact@1']:.6f}")
    print(f"  TokenF1@1:                  {results['TokenF1@1']:.6f}")
    print(f"  MeanCos@1(QSim):            {results['MeanCos@1(QSim)']:.6f}")
    print(f"  Semantic@1(ans_cos>=0.85):  {results['Semantic@1(ans_cos>=0.85)']:.6f}")
    if results["BERTScoreF1@1"] is None:
        print("  BERTScoreF1@1:              (pip install bert-score to enable)")
    else:
        print(f"  BERTScoreF1@1:              {results['BERTScoreF1@1']:.6f}")
    print(f"\n  Elapsed: {elapsed:.2f}s | Test samples: {n_test}")
    return results


# ================================================================
# INTERACTIVE DIALOG
# ================================================================
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("Dialog  |  'eval' ‚Üí –±–∞“ì–∞–ª–∞—É  |  'exit' ‚Üí —à—ã“ì—É")
    print("=" * 60)

    while True:
        user_input = input("\n–°“±—Ä–∞“õ: ").strip()
        if user_input.lower() == "exit":
            print("Stopped. üëã")
            break
        if user_input.lower() == "eval":
            evaluate_system()
            continue
        if not user_input:
            continue
        answer, _, qsim = ask_question(user_input, threshold=0.0)
        print(f"\n  –ñ–∞—É–∞–ø : {answer}")
        print(f"  QSim  : {qsim:.4f}")