# -*- coding: utf-8 -*-
"""Segment_Baseline_Strict_Fair_Comparison

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SLaL6ALsUxNi6FSAER9_9FDOwWqNqNg8
"""

!pip install bert-score

"""Dual View Segment Baseline Strict Fair Comparison"""

# @title
# ============================================================
# STRICT FAIR COMPARISON ✅
# BASE_CLEAN vs SEG_MORPH_ONLY vs SEG_DUAL_FUSED (clean+morph from segmented)
#
# - Loads TWO datasets:
#   (A) BASE (clean) dataset
#   (B) SEG (segmented) dataset with @@
# - Pairs records by CLEAN(question) key (answer-free key)
# - One deterministic split (seed) on the PAIRED intersection only
#
# MODE 1: BASE_CLEAN(FAIR_SPLIT)
#   - Index questions = CLEAN(base_question)
#   - Answers (gold/pred store) = CLEAN(base_answer)
#
# MODE 2: SEG_MORPH_ONLY(FAIR_SPLIT)
#   - Index questions = MORPH_MARKER(seg_question) (keeps @@)
#   - Answers stored = CLEAN(base_answer) ✅ strict fair: same gold space
#
# MODE 3: SEG_DUAL_FUSED(FAIR_SPLIT, alpha)
#   - q_clean = CLEAN(seg_question)        (removes @@)
#   - q_morph = MORPH_MARKER(seg_question) (keeps @@)
#   - q_fused = normalize(alpha*q_clean + (1-alpha)*q_morph)
#   - Answers stored = CLEAN(base_answer)  ✅ strict fair: same gold space
#
# Interactive QA: uses DUAL by default (can switch)
# Auto metrics AFTER exit:
#   Exact@1, TokenF1@1, MeanCos@1(QSim),
#   Semantic@1(ans_cos>=thr), BERTScore(optional)
# + CSV export per mode
# ============================================================

import os
os.environ["TRANSFORMERS_NO_TORCHVISION"] = "1"

import json, re, time, glob, random, csv
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
from sklearn.model_selection import train_test_split
from sentence_transformers import SentenceTransformer

try:
    from bert_score import score as bert_score
except Exception:
    bert_score = None


# ---------------------------
# CONFIG
# ---------------------------
BASE_PATH  = "baseline_15000.json"              # ✅ clean/baseline dataset
SEG_PATH   = "kazakh_segmented_15000.json"      # ✅ segmented dataset with @@

MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

SEED      = 42
TEST_SIZE = 0.10
SEM_THR   = 0.85
DEVICE    = None  # None -> auto

ALPHA     = 0.50  # dual-view fusion weight for CLEAN part

EXPORT_CSV        = True
CSV_PATH_BASE     = "fair_base_clean_details.csv"
CSV_PATH_MORPH    = "fair_seg_morph_only_details.csv"
CSV_PATH_DUAL     = "fair_seg_dual_fused_details.csv"


# ---------------------------
# Utility: file auto-find
# ---------------------------
def find_data_path(p: str) -> str:
    if Path(p).exists():
        return p
    candidates = [f"/content/{p}", f"/content/drive/MyDrive/{p}"]
    for c in candidates:
        if Path(c).exists():
            return c
    name = Path(p).name
    hits = glob.glob(f"**/{name}", recursive=True)
    if hits:
        return hits[0]
    near = glob.glob("**/*.json", recursive=True)
    raise FileNotFoundError(
        f"❌ File not found: {p}\nPWD: {Path.cwd()}\n"
        f"Found .json (first 30):\n" + "\n".join(near[:30])
    )


# ---------------------------
# Robust loader (JSON array / JSONL / brace-scan)
# ---------------------------
def load_qa_records(path: str) -> List[Dict[str, str]]:
    text = Path(path).read_text(encoding="utf-8", errors="ignore").strip()
    if not text:
        raise ValueError(f"Файл бос: {path}")

    if text[0] == "[":
        try:
            return _normalize_records(json.loads(text))
        except Exception:
            pass

    lines = [ln.strip().rstrip(",") for ln in text.splitlines() if ln.strip()]
    if lines and lines[0].startswith("{"):
        recs, ok = [], True
        for ln in lines:
            try:
                recs.append(json.loads(ln))
            except Exception:
                ok = False
                break
        if ok and recs:
            return _normalize_records(recs)

    objs, buf, depth = [], [], 0
    in_str, esc, started = False, False, False

    for ch in text:
        if not started:
            if ch == "{":
                started = True
                depth = 1
                buf = ["{"]
            continue

        buf.append(ch)

        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    obj_txt = "".join(buf)
                    buf = []
                    started = False
                    try:
                        objs.append(json.loads(obj_txt))
                    except Exception:
                        pass

    if not objs:
        raise ValueError(f"JSON оқу мүмкін болмады. Файл форматын тексеріңіз: {path}")
    return _normalize_records(objs)


def _normalize_records(data: Any) -> List[Dict[str, str]]:
    if not isinstance(data, list):
        raise ValueError("Дерек list болуы керек.")
    out = []
    for x in data:
        if not isinstance(x, dict):
            continue
        q = x.get("question") or x.get("instruction") or ""
        a = x.get("answer") or x.get("response") or ""
        q = str(q).strip()
        a = str(a).strip()
        if q and a:
            out.append({"question": q, "answer": a})
    if not out:
        raise ValueError("question/answer табылмады немесе бос.")
    return out


# ---------------------------
# Text normalization
# ---------------------------
_punct_space_left  = re.compile(r"\s+([.,!?;:%)\]\}])")
_punct_space_right = re.compile(r"([(\[\{])\s+")
_multi_space       = re.compile(r"\s+")

def _norm_space_punct(t: str) -> str:
    t = t.replace(" - ", "-")
    t = _punct_space_left.sub(r"\1", t)
    t = _punct_space_right.sub(r"\1", t)
    t = _multi_space.sub(" ", t).strip()
    return t

def morph_marker_view(text: str) -> str:
    """Keeps @@ exactly; only normalizes spaces/punct."""
    t = "" if text is None else str(text)
    return _norm_space_punct(t)

def clean_view(text: str) -> str:
    """Removes @@ markers + normalizes."""
    t = "" if text is None else str(text)
    t = t.replace("@@ ", "").replace("@@", "")
    return _norm_space_punct(t)

def norm_for_exact(text: str) -> str:
    return re.sub(r"\s+", " ", _norm_space_punct(str(text)).lower()).strip()

def tokens(text: str) -> List[str]:
    t = _norm_space_punct(str(text)).lower()
    return re.findall(r"[a-zA-Zа-яА-ЯәғқңөұүһіӘҒҚҢӨÚÜҺІ0-9]+", t)

def token_f1(pred: str, gold: str) -> float:
    p = tokens(pred); g = tokens(gold)
    if not p and not g: return 1.0
    if not p or not g: return 0.0
    from collections import Counter
    pc = Counter(p); gc = Counter(g)
    inter = sum((pc & gc).values())
    if inter == 0: return 0.0
    prec = inter / max(1, len(p))
    rec  = inter / max(1, len(g))
    return (2 * prec * rec) / (prec + rec + 1e-12)


# ---------------------------
# Embedding helpers
# ---------------------------
def _l2norm(x: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / n

def fuse_embeddings(e_clean: np.ndarray, e_morph: np.ndarray, alpha: float) -> np.ndarray:
    fused = alpha * e_clean + (1.0 - alpha) * e_morph
    if fused.ndim == 1:
        return fused / (np.linalg.norm(fused) + 1e-12)
    return _l2norm(fused)


# ---------------------------
# Pairing: STRICT FAIR intersection by CLEAN(question)
# ---------------------------
def build_paired_rows(base_rows: List[Dict[str,str]], seg_rows: List[Dict[str,str]]) -> List[Dict[str,str]]:
    base_map: Dict[str, Dict[str,str]] = {}
    for r in base_rows:
        k = clean_view(r["question"])
        if k and k not in base_map:
            base_map[k] = r

    seg_map: Dict[str, Dict[str,str]] = {}
    for r in seg_rows:
        k = clean_view(r["question"])
        if k and k not in seg_map:
            seg_map[k] = r

    keys = sorted(set(base_map.keys()) & set(seg_map.keys()))
    paired = []
    for k in keys:
        b = base_map[k]
        s = seg_map[k]
        paired.append({
            "base_q": clean_view(b["question"]),
            "base_a": _norm_space_punct(b["answer"]),      # gold space = base answer (clean)
            "seg_q_clean": clean_view(s["question"]),
            "seg_q_morph": morph_marker_view(s["question"]),
        })
    return paired


# ---------------------------
# Retrieval index
# ---------------------------
@dataclass
class QAIndex:
    mode: str
    q_text: List[str]
    q_emb: np.ndarray
    ans_text: List[str]
    a_emb: np.ndarray

def build_index_base_clean(model: SentenceTransformer, train_rows: List[Dict[str,str]]) -> QAIndex:
    q_view = [x["base_q"] for x in train_rows]
    a_view = [x["base_a"] for x in train_rows]
    q_emb  = model.encode(q_view, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    a_emb  = model.encode(a_view, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    return QAIndex("BASE_CLEAN(FAIR_SPLIT)", q_view, q_emb, a_view, a_emb)

def build_index_seg_morph_only(model: SentenceTransformer, train_rows: List[Dict[str,str]]) -> QAIndex:
    # questions: MORPH view only (keeps @@)
    q_morph = [x["seg_q_morph"] for x in train_rows]
    q_emb   = model.encode(q_morph, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

    # answers: STRICT FAIR = base answers (clean)
    a_view  = [x["base_a"] for x in train_rows]
    a_emb   = model.encode(a_view, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

    return QAIndex("SEG_MORPH_ONLY(FAIR_SPLIT)", q_morph, q_emb, a_view, a_emb)

def build_index_dual_fused_from_seg(model: SentenceTransformer, train_rows: List[Dict[str,str]], alpha: float) -> QAIndex:
    q_clean = [x["seg_q_clean"] for x in train_rows]
    q_morph = [x["seg_q_morph"] for x in train_rows]

    e_clean = model.encode(q_clean, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    e_morph = model.encode(q_morph, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
    q_fused = fuse_embeddings(e_clean, e_morph, alpha)

    a_view = [x["base_a"] for x in train_rows]
    a_emb  = model.encode(a_view, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

    q_text = [f"CLEAN||MORPH: {qc} || {qm}" for qc, qm in zip(q_clean, q_morph)]
    return QAIndex(f"SEG_DUAL_FUSED(alpha={alpha}, FAIR_SPLIT)", q_text, q_fused, a_view, a_emb)

def retrieve_top1(index: QAIndex, q_vec: np.ndarray) -> Tuple[int, float]:
    sims = np.dot(index.q_emb, q_vec)
    i = int(np.argmax(sims))
    return i, float(sims[i])


# ---------------------------
# BERTScore helper
# ---------------------------
def _bert_lang_try(preds: List[str], golds: List[str]) -> Optional[float]:
    if bert_score is None:
        return None
    for lang in ("kk", "tr", "en"):
        try:
            P, R, F1 = bert_score(preds, golds, lang=lang, rescale_with_baseline=True)
            arr = F1.numpy() if hasattr(F1, "numpy") else np.array(F1)
            return float(np.mean(arr))
        except Exception:
            continue
    return None


# ---------------------------
# Evaluation (generic for any index mode) — gold answers are base_a
# ---------------------------
def eval_with_index(model: SentenceTransformer, index: QAIndex, test_rows: List[Dict[str,str]], alpha: float) -> Tuple[Dict[str,Any], List[Dict[str,Any]]]:
    # Build test query embeddings matching each mode
    if index.mode.startswith("BASE_CLEAN"):
        test_q = [x["base_q"] for x in test_rows]
        test_q_emb = model.encode(test_q, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

    elif index.mode.startswith("SEG_MORPH_ONLY"):
        test_q = [x["seg_q_morph"] for x in test_rows]
        test_q_emb = model.encode(test_q, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

    else:
        tq_clean = [x["seg_q_clean"] for x in test_rows]
        tq_morph = [x["seg_q_morph"] for x in test_rows]
        e_clean = model.encode(tq_clean, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
        e_morph = model.encode(tq_morph, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)
        test_q_emb = fuse_embeddings(e_clean, e_morph, alpha)
        test_q = tq_morph  # for logging

    gold = [x["base_a"] for x in test_rows]
    gold_a_emb = model.encode(gold, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

    exacts, tf1s, qcos1s, semhit = [], [], [], []
    preds_for_bert, golds_for_bert = [], []
    details = []

    for i in range(len(test_rows)):
        idx, qcos = retrieve_top1(index, test_q_emb[i])
        pred = index.ans_text[idx]
        g    = gold[i]

        ex   = 1.0 if norm_for_exact(pred) == norm_for_exact(g) else 0.0
        f1   = token_f1(pred, g)
        qsim = float(qcos)

        sem_cos = float(np.dot(index.a_emb[idx], gold_a_emb[i]))
        sh = 1.0 if sem_cos >= SEM_THR else 0.0

        exacts.append(ex); tf1s.append(f1); qcos1s.append(qsim); semhit.append(sh)

        if bert_score is not None:
            preds_for_bert.append(pred)
            golds_for_bert.append(g)

        details.append({
            "mode": index.mode,
            "test_question": test_q[i],
            "gold_answer": g,
            "pred_answer": pred,
            "QSim": qsim,
            "Exact": ex,
            "TokenF1": f1,
            "AnsCos": sem_cos,
            "SemHit": sh
        })

    out = {
        "Mode": index.mode,
        "Exact@1": float(np.mean(exacts)),
        "TokenF1@1": float(np.mean(tf1s)),
        "MeanCos@1(QSim)": float(np.mean(qcos1s)),
        f"Semantic@1(ans_cos≥{SEM_THR})": float(np.mean(semhit)),
    }

    if bert_score is not None and preds_for_bert:
        bf1 = _bert_lang_try(preds_for_bert, golds_for_bert)
        if bf1 is not None:
            out["BERTScoreF1@1"] = float(bf1)

    return out, details


# ---------------------------
# Pretty print + export
# ---------------------------
def print_result_table(rows: List[Dict[str,Any]]):
    print("\n==================== RESULTS (STRICT FAIR COMPARISON) ====================")
    keys = []
    for r in rows:
        for k in r.keys():
            if k not in keys:
                keys.append(k)

    for r in rows:
        print("\n---", r.get("Mode", "MODE"), "---")
        for k in keys:
            if k not in r:
                continue
            v = r[k]
            if isinstance(v, float):
                print(f"{k:>28}: {v:.6f}")
            else:
                print(f"{k:>28}: {v}")

def export_csv(details: List[Dict[str,Any]], path: str):
    if not details:
        return
    fields = list(details[0].keys())
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        for r in details:
            w.writerow(r)
    print(f"\n✅ CSV exported: {path}  (rows={len(details)})")


# ---------------------------
# Interactive QA (uses DUAL by default)
# ---------------------------
def interactive(model: SentenceTransformer, idx_base: QAIndex, idx_morph: QAIndex, idx_dual: QAIndex, alpha: float):
    mode = "DUAL"  # default
    print("\n==================== INTERACTIVE QA (STRICT FAIR) ====================")
    print("Commands: /base  -> BASE_CLEAN mode")
    print("          /morph -> SEG_MORPH_ONLY mode")
    print("          /dual  -> SEG_DUAL_FUSED mode")
    print("          exit   -> finish and run metrics\n")

    while True:
        q = input("Сұрақ: ").strip()
        if not q:
            continue
        if q.lower() in {"exit","quit","q"}:
            break
        if q.lower() == "/base":
            mode = "BASE"
            print("✅ Switched to BASE_CLEAN\n")
            continue
        if q.lower() == "/morph":
            mode = "MORPH"
            print("✅ Switched to SEG_MORPH_ONLY\n")
            continue
        if q.lower() == "/dual":
            mode = "DUAL"
            print(f"✅ Switched to SEG_DUAL_FUSED(alpha={alpha})\n")
            continue

        if mode == "BASE":
            qv = model.encode([clean_view(q)], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)[0]
            j, sim = retrieve_top1(idx_base, qv)
            print(f"\n[{idx_base.mode}] Top1 QSim={sim:.4f}\n{idx_base.ans_text[j]}\n")

        elif mode == "MORPH":
            qv = model.encode([morph_marker_view(q)], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)[0]
            j, sim = retrieve_top1(idx_morph, qv)
            print(f"\n[{idx_morph.mode}] Top1 QSim={sim:.4f}\n{idx_morph.ans_text[j]}\n")

        else:
            qc = clean_view(q)
            qm = morph_marker_view(q)
            ec = model.encode([qc], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)[0]
            em = model.encode([qm], convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)[0]
            qf = fuse_embeddings(ec, em, alpha)
            j, sim = retrieve_top1(idx_dual, qf)
            print(f"\n[{idx_dual.mode}] Top1 QSim={sim:.4f}\n{idx_dual.ans_text[j]}\n")


# ---------------------------
# MAIN
# ---------------------------
def main():
    random.seed(SEED)
    np.random.seed(SEED)

    base_path = find_data_path(BASE_PATH)
    seg_path  = find_data_path(SEG_PATH)

    base_rows = load_qa_records(base_path)
    seg_rows  = load_qa_records(seg_path)

    print(f"[BASE] Loaded: {len(base_rows)} | {base_path}")
    print(f"[SEG ] Loaded: {len(seg_rows)}  | {seg_path}")

    paired = build_paired_rows(base_rows, seg_rows)
    if not paired:
        raise ValueError("❌ PAIRED intersection бос. CLEAN(question) бойынша қиылыс табылмады.")

    print("\n==================== STRICT PAIRING ====================")
    print(f"Paired intersection size = {len(paired)} (by CLEAN(question))")

    train_rows, test_rows = train_test_split(
        paired, test_size=TEST_SIZE, random_state=SEED, shuffle=True
    )

    print("\n==================== ONE FAIR SPLIT ====================")
    print(f"Total(PAIRED)={len(paired)} | Train={len(train_rows)} | Test={len(test_rows)} | seed={SEED} | test_size={TEST_SIZE}")
    print("Mode A = BASE_CLEAN(FAIR_SPLIT)")
    print("Mode B = SEG_MORPH_ONLY(FAIR_SPLIT)")
    print(f"Mode C = SEG_DUAL_FUSED(clean+morph from SEG), alpha={ALPHA} (FAIR_SPLIT)")
    print("Gold answers = BASE answers (clean) ✅")

    model = SentenceTransformer(MODEL_NAME, device=DEVICE)
    print(f"\nModel: {MODEL_NAME}")

    print("\n[1/4] Building BASE_CLEAN index...")
    idx_base = build_index_base_clean(model, train_rows)

    print("[2/4] Building SEG_MORPH_ONLY index...")
    idx_morph = build_index_seg_morph_only(model, train_rows)

    print("[3/4] Building SEG_DUAL_FUSED index...")
    idx_dual = build_index_dual_fused_from_seg(model, train_rows, ALPHA)

    # Interactive
    interactive(model, idx_base, idx_morph, idx_dual, ALPHA)

    # Auto eval after exit
    print("\n[4/4] Running evaluation (STRICT FAIR)...")
    t0 = time.time()
    res_b, det_b = eval_with_index(model, idx_base, test_rows, ALPHA)
    res_m, det_m = eval_with_index(model, idx_morph, test_rows, ALPHA)
    res_d, det_d = eval_with_index(model, idx_dual, test_rows, ALPHA)
    dt = time.time() - t0

    print_result_table([res_b, res_m, res_d])
    print(f"\nTime: {dt:.2f}s")
    if bert_score is None:
        print("Note: BERTScore орнатылмаған (pip install bert-score).")

    if EXPORT_CSV:
        export_csv(det_b, CSV_PATH_BASE)
        export_csv(det_m, CSV_PATH_MORPH)
        export_csv(det_d, CSV_PATH_DUAL)

    print("\n✅ DONE: STRICT FAIR COMPARISON finished (paired by CLEAN(question), same split, same seed, same gold)")

if __name__ == "__main__":
    main()